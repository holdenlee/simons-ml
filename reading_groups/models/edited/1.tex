\section{Big picture}

We'll talk about several results which have different names in different fields. %guises 
You probably know them, but don't know the same or related idea comes up in the other fields.\\

\begin{tabular}{|p{25mm}|p{25mm}|p{25mm}|p{25mm}|p{25mm}|p{1mm}|}
\hline 
 & Boosting & Hard-core lemma & Dense model theorem & Weak regularity & ?\\
\hline 
Area & ML & CC,  Derandom-ization & Additive combinatorics, CC & Graph theory & \\
\hline 
Credit & Shapiro, Freund-Schapire & Impagliazzo, Holenstein & Green-Tao, Barak-Shaltiel-Wigderson & Szemeredi, Frieze-Kannan & \\
\hline 
Get & Circuit computing $f$ $1-\delta$ of the time & '' & Proof that set isn't $\delta$-dense & '' & \\
\hline 
Unless & Weak learner fails on distribution of density $\Omega(\delta)$ & Hard-core distribution  & $\Omega(\delta)$-dense ``model'' indistinguishable from set & A model succinctly describing set & \\
\hline 
Algorithm needed %to be const.
 & Weak learner & '' & Distinguisher & '' & \\
\hline 
\end{tabular}

We will take these theorems that we know to be true and show implications between them. Implications are due to...
\begin{enumerate}
\item
Boosting$\implies$Hard-core: 
Klivans and Servedio.
\item
Hard-core$\implies$Dense model:
Impagliazzo
\item Dense model$\implies$Weak regularity:
Trevisan-Tulsiani-Vadhan, Reingold-Trevisan-Tulsiani-Vadhan
\item Weak regularity$\implies$boosting: Trevisan-Tulsiani-Vadhan
\end{enumerate}â€¢

What can we gain from looking at these connections?
\begin{enumerate}
\item
Versatility: We can ``retrofit'' algorithms for one setting to get algorithms for the other settings.

For example, there are many boosting algorithms. When you follow this progression, you get different quantitative and qualitative versions of dense model theorem and regularity.
\item
Algorithmic and constructive results: 

There are nonconstructive versions using the min-max theorem for boosting, hard-core lemma, dense model theorem. We care about algorithmic versions. 

Note that the algorithmic result that we care about is different in the different settings.  In ML we care about getting a function that computes a function much of the time. On the other side, we're really after the distribution where the weak learner fails, so that we get a model that succinctly describes the set. 

We pay attention to do the reductions in an algorithmic, not just an existential way.
\item
Using the dense model theorem for learning. Can we take a boosting technique and use it in an unsupervised way?
%(The dense model theorem can be thought of as giving a way to represent a dataset succinctly.)
\item
Generality: some things seem to be specific to a setting (density of graphs). 

But actually, weak regularity doesn't have anything to do with graphs being dense. We can relativize it to subgraphs of any graph. You can look at subgraphs of expanders, bipartite graphs, etc., and plug it in the same machinery. Likewise if you want to look at spectral norms rather than cuts.
\end{enumerate}

%\section{Intuition}

Here is a cartoon:

\begin{enumerate}
\item
Let $X$ be a set, e.g. a distribution of points in the square. %in $\R^n$.
%a distribution of points in the square.

Let $S$ be some distribution on points in $X$.

\ig{pic1}{0.5}

Let $\mathcal T$ be a set of classifiers, ex. a set of half-planes.

Let $\mathcal F_K\mathcal T$ be boolean functions on $K$ functions in $\mathcal T$; here, partitions into polygonal regions by $k$ half-planes.

We want to pre-process the distribution to be able to answer queries in $\mathcal F_K\mathcal T$.
\item
A violation of pseudo-density in this setting means there is a polygonal region with many more points from $S$ than its volume, a ``hot spot''.


\ig{pic2}{0.5}

$$
\text{Area}(\text{region})< \delta \Pr_S (\text{region})-\varepsilon.
$$
%relatively small poly with few sides, vastly overrepresented. 
\item
A model is a partition into polygonal regions, with a probability distribution on regions. A simple model is defined by at most $k$ lines. 

The property of a model is that we can estimate half-space probabilities (``given any half-space, what proportion of points of $S$ are on one side of it?'') by treating the points as if uniform within regions.

\ig{pic3}{0.5}

%Uniformly sample within region. 
%``Given any half-space, how many points are on one side of it?''
%triple number of lines faces
\item
The algorithmic requirement in order to process the points to answer queries in $\mathcal F_K\mathcal T$ is:  given a set of points sampled from $S$, and a set of points sampled from $U$, find a half-space that approximately maximizes the difference in probabilities for these two sets. The equivalent in boosting is a distinguishing algorithm.
%When using boosting, the equivalent is a distinguishing algorithm.
%Find a separating halfspace that approximately maximizes the probability distribution under the two distributions.
%low vc dimension: best line for finite passes through 2 points. move until...
%poly whose degree is dimension. 
%If you have  a hypo for more complicated things
\end{enumerate}
%can have small density but still be pseudo-dense wrt universe
%maybe not realized as part of general picture.
%``Using boosting in unsupervised context'': not told what we want to learn about distribution. Just have model to address queries later on. You need tests in mind.\\ %the category of things y

\noindent
\begin{tabular}{|p{20mm}|p{35mm}|p{35mm}|p{35mm}|p{15mm}|}
\hline 
Setting & Boosting & Hard-core measure & DMT/transference principle & Weak regularity\\

\hline 
 & WL: $|\mu_{i}|\ge2\delta$, $\mu_{i}=g(h_{1,}\ldots,h_{i},f)$, $h_{i+1}\in\mathcal{T}$, $k$
iterations & Hardcore measure: $\mu_{k}=g(h_{1},\ldots,h_{k},f)$, $|\mu_{k}|\ge2\delta$ & Model: $\mu_{k}=g(h_{1},\ldots,h_{k},o)$, $|\mu_{k}|\ge\delta$& \\ 
\hline 
 & SL: $H=G(h_{1},\ldots,h_{k})$, $\Pr[H=f]\ge1-\delta$ & Violation of hardness: $H=G(h_{1},\ldots,h_{k})$, $\Pr[H=f]>1-\delta$ & Violation of pseudo-density $H=G(h_{1},\ldots,h_{k})$, $H(U)\le\delta H(S)-\varepsilon$& 
 \\
\hline 
Assumption & WL never fails & Violation is impossible & Violation of pseudo-density is impossible
& Actually dense \\
\hline 
Conclusion & SL works & Hard-core measure exists, with same $k$, $G$, $g$ & Model exists & Model exists
\\
\hline 
Algorithmic & Weak learner requirement & Approximately optimal weak learner & Approximately optimal distinguisher & \\
\hline 
\end{tabular}

\noindent
\begin{tabular}{|p{20mm}|p{35mm}|p{35mm}|p{35mm}|p{15mm}|}
\hline 
Setting & Boosting & Hard-core measure & DMT/transference principle & Weak regularity\\

\hline 
 & WL: $|\mu_{i}|\ge2\delta$, $\mu_{i}=g(h_{1,}\ldots,h_{i},f)$, $h_{i+1}\in\mathcal{T}$, $k$
iterations & Hardcore measure: $\mu_{k}=g(h_{1},\ldots,h_{k},f)$, $|\mu_{k}|\ge2\delta$ & Model: $\mu_{k}=g(h_{1},\ldots,h_{k},o)$, $|\mu_{k}|\ge\delta$& \\ 
\hline 
 & SL: $H=G(h_{1},\ldots,h_{k})$, $\Pr[H=f]\ge1-\delta$ & Violation of hardness: $H=G(h_{1},\ldots,h_{k})$, $\Pr[H=f]>1-\delta$ & Violation of pseudo-density $H=G(h_{1},\ldots,h_{k})$, $H(U)\le\delta H(S)-\varepsilon$& 
 \\
\hline 
Assumption & WL never fails & Violation is impossible & Violation of pseudo-density is impossible
& Actually dense \\
\hline 
Conclusion & SL works & Hard-core measure exists, with same $k$, $G$, $g$ & Model exists & Model exists
\\
\hline 
Algorithmic & Weak learner requirement & Approximately optimal weak learner & Approximately optimal distinguisher & \\
\hline 
\end{tabular}


Some comments:

\begin{enumerate}
\item
Boosting: %$\delta$ is the same.
Note it's important that the $\delta$ here is the same; many boosting algorithms meet this criterion.
%twist

The theorem says that ``either weak learner fails or strong learner works.'' 

In boosting, we think of weak learner as never failing.

\item Hard-core measure lemma:  The lemma says that either we can find hard-core
measure, on which no function can compute the function $f$ more than $1/2 +\delta$
of time; or find a function computing $f$ $1-\delta$ of the time.

Here, we want to come up with the measure. Although the logical format is the same as boosting, here we assume that the violations never happen (there is no strong learner).
%The only change is psychological: we assume violations never happen. The logical format is the same.

Every boosting algorithm gives hard-core measure lemma with the same parameters, and with exactly the same way of ``gluing'' the functions. 
%the functions is exactly the same. 
%get function computes $f$ or hard-core measure.

%We have different hard-core measure lemmas with different gluing. 
Sometime you care about computational complexity of $G$ but not of $g$, or vice versa. 
%, etc. Or it could be the other way around.
\item
We can convert the hard-core measure theorem into the dense model theorem/transference principle (Tao and Ziegler).

Here, we have a distribution we're trying to model. 
Either the distribution has pseudo-density property--- there isn't a violation that's definable from $k$ different properties from hypothesis class, where violation means that the expected value is much smaller on $U$ than on $S$---or we get a model of density $\ge \delta$. %Even if $\delta$-dense, the best we
%Get model definable in terms of $K$ hypotheses from class. 

Assuming that violation of pseudo-density does not happen, we get a model.
\item
Weak regularity is just DMT except the distribution actually is dense. It's not so interesting that it has a dense model. 

What we get is that the dense model you get is simple, definable in terms of a small number of basic hypotheses. 

%applying to actually dense, so you don't worry about violation of pseudo-density.
Sometimes we care about simplicity in the model, and sometimes simplicity in $G$.
%point is to have a delta: HC
%and point is to be a delta.
\item
Note the $k$ is the same throughout. Reductions preserves $k$, and the functions $h_i, G$. 

We don't only have the fact that boosting implies hard-core lemma implies regularity lemma. We have the stronger result that whatever boosting algorithm you give me, I get a hard-core lemma and regularity lemma with the same parameters and algorithm. %We can immediately jump to whatever we're interested in.
Thus we can pick the boosting algorithm that gives the best results for our application.
\end{enumerate}
\section{Setup}
First we discuss the PAC learning model.

%Let $U$ be the universe (a big set of possible inputs) and $f:U\to \{0,1\}$ be a boolean function. 
Let $U$ be a set, and by abuse of notation, also a distribution on that set. (Think of $U$ as the universe, the set of possible inputs.) For simplicity, take the distribution to be uniform.
%distribution on a set $X$, and 
Let $f:U\to \{0,1\}$ be a boolean function.
A learning algorithm can request any number of points $(x,f(x))$ where $x\sim U$. The goal is to find a hypothesis $h$ such that 
$$
\Pr_{x\sim U} [h(x)=f(x)]\ge 1-\delta.
$$
\begin{thm}
A \vocab{strong learner} for $(U,f)$ with hypothesis class $\mathcal H$ is an algorithm such that given samples $(x,f(x)), x\sim U$, outputs $h\in \mathcal H$ (with high probability) such that 
$$\Pr_{x\sim U}[h(x)=f(x)]\ge 1-\delta.$$

(Typically, we say that the probability of success is $1-\varepsilon$, ask for a strong learner for all $f\in \mathcal F$, and require it to run in time $\operatorname{poly}(1/\varepsilon, 1/\delta)$.) 
\end{thm}

In boosting, we assume that we have weak learners.
\begin{thm}
A $\varepsilon$-\vocab{weak learner} for $(\mu, f)$ with hypothesis class $\mathcal H$ is an algorithm such that given $(x,f(x)), x\sim \mu$, outputs $h$ (with high probability) such that
$$
\Pr_{x\sim \mu} [h(x)=f(x)] \ge \frac{1}{2}+\varepsilon.
$$
\end{thm}
It only has to output a function that is somewhat correlated with the right answer.
Typically, we ask the weak learner to work on any distribution $\mu$ satisfying some assumptions.

In order to use a weak learner, we construct a routine that subsamples the distribution $U$ to pass to pass to the weak learner.
\begin{df}
Let $\mu:U\to [0,1]$. Define the probability distribution\footnote{When $U$ is not uniform and has distribution $u(x)$, this is $\frac{\mu(x)u(x)}{\sum_{x'\in U} \mu(x')u(x')}$.}
$$
D_\mu(x) = \frac{\mu(x)}{\sum_{x'\in U}\mu(x')}.
$$
\end{df}
Think of this as rejection sampling: pick $x\sim U$, keep it with probability in $[0,1]$, or else throw if back and repeat.

In order for this sampling to be efficient, we need $\mu$ to not be too small.
\begin{df}
Define the \vocab{density} of $\mu$ in $U$ to be
$$|\mu| = \operatorname{E}_{x\in U} \mu(x).$$ 
\end{df}
%Think of this as a notion of density. 

We will use weak learners in the following context. 
\begin{enumerate}
\item
We will only run weak learners on distributions whose density is not too small (the dependence on $\delta$ is $|\mu|=\Omega(\delta)$). We don't want to run a weak learner on a distribution of very low density, because the time to simulate  the distribution is inversely proportional to the density.
\item
We ask the weak learners to output a function in a given class $h\in \mathcal T$. 

Then it will turn out that that both the measures that we run the weak learners on, and the final hypothesis, will be describable using $\mathcal F_l \mathcal T$ (see below), for some class $\mathcal F$.
\end{enumerate}

\begin{df}
Say that a set $\mathcal T$ of functions $U\to \{0,1\}$ form a class if $f\in \mathcal T$ implies $1-f \in \mathcal T$. 

Let $\mathcal F$ be a class of boolean functions. Define the class of functions
$$
\mathcal F_k \mathcal T = \{f(h_1(x),\ldots, h_k(x)) : f\in\mathcal F, h_1,\ldots, h_k\in \mathcal T\}.
$$
\end{df}


\section{Boosting and the Hard-core lemma}

%If you just care about ML we don't care about this details, but to go further, we do.

The first boosting algorithm we give is totally ridiculous from the ML point of view. For people who work on weak regularity on graphs this is the natural version, and leads to the standard versions of results.

We will take $\mathcal F$ to be the set of all boolean functions, so given hypotheses $h_1,\ldots, h_k$, we can choose the best predictor using $h_1(x),\ldots, h_k(x)$.

\begin{thm}[Boosting with decision trees]\label{thm:boosting}
Let $U$ be a distribution, $\mathcal T$ a class of boolean functions $U\to \{0,1\}$,  $\mathcal F$ the class of all boolean functions. 
Let $f:U\to \{0,1\}$ be a given function (which we are trying to learn).

\begin{enumerate}
\item
Suppose that there is a $\delta$-weak learner such that given any distribution $\mu$ on $U$ with $|\mu|\ge 2\delta$, it produces $h\in \mathcal T$ such that 
$$
\Pr_{x\sim \mu} [h(x) = f(x)] \ge \frac{1}{2}+ \varepsilon.
$$
%with hypothesis class $\mathcal H$ 
\item
Then there is a strong learner that produces $h\in \mathcal F_k\mathcal T$ with $k\le\lceil 1/\varepsilon^2\delta^2\rceil$ such that\footnote{
We ignore sample complexity here. In reality, because we only see $U$ from samples, we need to think about generalization. If the VC-dimension of $\mathcal T$ is $d$, then the VC-dimension of $\mathcal F_k\mathcal H$ is at most $k^d$. In ML we don't want to take $\mathcal F$ to be the class of all boolean functions. For this theorem, let's just assume we are actually given all pairs $(x,f(x))$.
}
$$
\Pr_{x\sim U} [h(x) = f(x)]\ge 1-\delta.
$$
\end{enumerate}
\end{thm}

\begin{thm}[Hard-core lemma]
\label{thm:hardcore}
Let $U$ be a distribution, $\mathcal T$ a class of boolean functions $U\to \{0,1\}$, $\mathcal F$ the class of all boolean functions.

Then either
\begin{enumerate}
\item %(Function is \fixme{...})
There exists $h\in \mathcal F_k \mathcal T$ such that 

$$
\Pr_{x\sim U} [h(x)=f(x)] \ge 1-\delta,
$$

where $k \le 1/\varepsilon^2\delta^2$, or
\item (There exists a hard-core distribution.)
There exists $|\mu|\ge 2\delta$ on $U$, such that for all $h\in \mathcal T$, 
$$
\Pr_{x\sim \mu}[h(x) = f(x)] \le \frac{1}{2}+\varepsilon.
$$
\end{enumerate}
\end{thm}

Note it is important for us to keep track of the size of the hardcore distribution, which is $\ge 2\delta$ here.
Different boosting algorithms will give the result for different classes of functions $\mathcal F$.

\begin{proof}[Proof of hard-core lemma~\ref{thm:hardcore} from boosting~\ref{thm:boosting}]
Let weak learner be exhaustive search over $\mathcal T$. The weak learner operates on distributions $|\mu_i|\ge 2\delta$. If it always produces $h_i$ with bias $\ge \delta$, then continue  and obtain the strong learner: we get some $H\in \mathcal F_k \mathcal T$ such that  $H(x)=f(x)$ with probability $1-\delta$. 

If at some step $i$ our exhaustive search algorithm gets stuck, we get a distribution $\mu_i$ that's hard-core.
\end{proof}




\section{Dense model theorem}

\begin{df}
For a set $S\subseteq U$ and a function $T:U \to \{0,1\}$,  let $T(S):=\operatorname{E}_{x\in S} T(x)$. (For a measure $\mu: U\to [0,1]$, also write $T(\mu) = \operatorname{E}_{x\sim \mu} T(x)$.)

Let $S\subseteq U$ be a subset, and let $\mathcal T$ be a set of tests. $S$ is \vocab{$(\varepsilon,\delta)$-pseudo-dense against $\mathcal T$} if for all $T\in \mathcal T$, 
$$
T(U) \ge \delta T(S)-\varepsilon.
$$
\end{df}
Think of saying that the tests $\mathcal T$ don't reveal that the set $S$ is small. 
\begin{enumerate}
\item
One way of being pseudo-dense is to actually be dense.
\item
Another, one step removed, is that there's a set $R$ (or more generally, a measure $\mu$) that's
indistibguishable from $S$ by $\mathcal T$, and such that $R$ occupies at least a $\delta$ fraction of $U$.
% indistinguishable from the whole distribution (in the sense of being at least about $\delta$ fraction of it), and the set is dense in $R$.
\end{enumerate}â€¢
\begin{df}
For two distributions $\mu_1,\mu_2$ on $U$, we say that $\mu_1,\mu_2$ are indistinguishable by tests in  $\mathcal T$ up to $\varepsilon$, written $\mu_1 \sim_{\mathcal T} \mu_2$ within $\varepsilon$, if for every $T\in \mathcal T$, 
$$
|\operatorname{E}_{\mu_1}T - \operatorname{E}_{\mu_2} T| \le \varepsilon.
$$
\end{df}
\begin{thm}[Dense model theorem]\label{thm:dmt}
Let $\mathcal T$ be a class of tests $U\to \{0,1\}$. 

If $S$ is $(\varepsilon,\delta)$-pseudodense against $F_k\mathcal T$, $k=O(1/\varepsilon^2\delta^2)$ then there exists $\mu$, $\mu\in F_k\cal T$ such that $|\mu|\ge \frac{\delta}{1+\delta}-O(\varepsilon)$ and $D_\mu\sim_{\cal T} S$ to within $O(\varepsilon/\delta)$. 
\end{thm}
The idea in the proof is to use the Hard-core lemma, with the hard function being membership in $S$.
\begin{proof}
Let $U'$ be the following distribution: let $\delta'=\frac{\delta}{1+\delta}$ and 
\begin{enumerate}
\item
with probability $\delta'$, take $x\in S$ and output $(0,x)$
\item
with probability $1-\delta'$, take $x\in U$ and output $(1,x)$.
\end{enumerate}
Define a test $T\in \mathcal T$ to operate on an example $(y, x)$ by $T(y,x)=T(x)$. For $T\in \mathcal F_k \mathcal T$, 
\begin{align}
\Pr_{U'}[T((y,x))=y] &= \delta' T(S) + (1-\delta') (1-T(U)) \\
   &= 1-\delta' + \delta' (T(S)) - (1-\delta') T(U)\\
   &= 1-\delta' + \frac{1}{1+\delta} (\delta T(S) - T(U))\\
   &\le 1-\delta'+\varepsilon.
\end{align}

No test in $\mathcal F_k \mathcal T$ can be correct with probability $>\delta'-\varepsilon$. By the Hard-core Lemma~\ref{thm:hardcore}, there exists $|\mu'|\ge 2(\delta'-\varepsilon)$ such that for any $T\in \mathcal T$, $\Pr_{(x,y) \sim U'}[T(x)=y]\le  \frac{1}{2}+\varepsilon$.

In order for $\mu'$ to be hardcore, it must be split approximately evenly between $U$ and $S$ (up to $\varepsilon$); otherwise; we could have an advantage by predicting constant 0 or 1. 
Thus each part has at least $2(\delta'-\varepsilon) (1/2 - \varepsilon) = \delta'(1-O(\varepsilon/\delta))$ of the mass. Then
$$D_{\mu'|_U}\sim_{O(\varepsilon)} D_{\mu'|_S}\sim_{O(\varepsilon/\delta)} S.$$
%. Now 
%$$\operatorname{E}_{(y,x)\sim U'} \mathbf{1}_{S}(x)\mu'(x) \ge 2(\delta'-\varepsilon)(\rc 2- \varepsilon') \ge \delta\pa{1-O(\delta)-O\pf{\varepsilon}{\delta}}$$ 
%so $D_{\mu'|_S}\sim_{O\pf{\varepsilon}{\delta}} S$. 
%Putting this together gives $D_{\mu'|_U} \sim_{O\pf{\varepsilon}{\delta}} S$.
\end{proof}

\section{Proof for boosting}

\begin{proof}[Proof of Theorem~\ref{thm:boosting}]
%any tests, underlying dist, boosting alg
%applications of strong regularity going other way?
%can use to prove strong reg, but don't know generic
%how to define properties particular algorithm has and see if it's interesting for boosting itself. 
%not so great as a boosting algorithm, although other assm make it feasible

The algorithm is as follows. Let $WL(\mu)$ denote the weak learner operating on $(\mu, f)$.

Let $\mu_0$ be constant 1, $i=0$.

While $|\mu_i|\ge 2\delta$, do
\begin{itemize}
\item
$h_{i+1}\mapsfrom WL(\mu_i)$.
\item
Partition $U$ according to values of $h_1,\ldots,h_i$.

Let $h_{1:i}(x):= (h_1(x),\ldots, h_i(x))\in \{0,1\}^i$, and let $B_i(x)$ be the ``block'' that $x$ is in, 
$$B_i(x) = h_{1:i}^{-1}(h_{1:i}(x)) = \{y\in U : h_{1:i}(x)=h_{1:i}(y)\}.$$ 

%For a function $f$, let $\operatorname{Maj}(f)$ denote the majority value of $f$.
For a set $B$, let $\operatorname{Maj}(B)$ denote the majority value of $f$ on $B$. 
\item
Define $\mu_{i+1}$ by 
%
$$\mu_{i+1}(x) = \begin{cases}
\frac{1-p_{\operatorname{Maj}, B_i(x)}}{p_{\operatorname{Maj}, B_i(x)}},&\text{if } f(x)=\operatorname{Maj}(B_i(x))\\
1,&\text{otherwise}
\end{cases}â€¢ $$

%where $p_{\operatorname{Maj}} = \Pr(h_{i+1}(x)= \operatorname{Maj}(h_{i+1}))$.
where $p_{\operatorname{Maj},B} = \Pr(f(y) = \operatorname{Maj}(B)| y\in B)$, the proportion of the majority in $B$.

\item
$i\mapsfrom i+1$.
\end{itemize}
Finally, return $H_{i}(x) = \operatorname{Maj}({B_{i}(x)})$, i.e., look at the block that $x$ is in, and choose the majority value.

Note that the measure $\mu_{i+1}$ rebalances %so that $\Pr_{\mu_{i+1}}(h_{i+1}(x) = 1)= \Pr_{\mu_{i+1}}(h_{i+1}(x)=0)=\rc2.$
each block $B_i$ such that conditioned on $y$ being in a block $B_i(x)$, 
$$\Pr_{y\sim \mu_{i+1}}(f(y)=1|y\in B_i(x)) = \Pr_{y \sim \mu_{i+1}}(f(y)=0|y\in B_i(x))=\frac{1}{2}.$$

Indeed, we have

\begin{align}
\operatorname{E}_{y\sim U}[\mathbf{1}_{f(y)=1} 
%\sum_{y\in B_i(x), f(y)=1}
 \mu_{i+1}(y) |y\in B_i(x)]
 &=p_{\operatorname{Maj}, B_i(x)} \frac{1-p_{\operatorname{Maj}, B_i(x)}}{p_{\operatorname{Maj}, B_i(x)}} = 1-p_{\operatorname{Maj}, B_i(x)}\\
 \operatorname{E}_{y\sim U}[\mathbf{1}_{f(y)=0} 
%\sum_{y\in B_i(x), f(y)=1}
 \mu_{i+1}(y) |y\in B_i(x)]
 &=\left(1-p_{\operatorname{Maj}, B_i(x)}\right)\cdot 1 = 1-p_{\operatorname{Maj}, B_i(x)}\\
|\mu_{i+1}| =  \operatorname{E}_{y\sim U}[
 \mu_{i+1}(y)]&=\sum_{\text{block }B_i} [2(1-p_{\operatorname{Maj}, B_i}) \Pr(B_i)]\\
 & \ge 2(1-p_{\operatorname{Maj},U}).
\end{align}

%Note that $p_{\operatorname{Maj}} \ge 1-\delta$ iff for $x$ in the majority, $\mu_{i+1}(x) \le \frac{\delta}{1-\delta}$. Thus if 
Note that if $|\mu_{i+1}|\le 2\delta$, then $\Pr_{x\in X}[H_i=f]\ge 1-\delta$, and we are done. (We stop before we have to apply the weak learner to a distribution of density $<\delta$.)

We need to show this method terminates in a bounded number of steps.

Consider the potential function
$$
\varphi_i = \operatorname{E}_{x\sim U} [(\Pr[f=1|B_i(x)])^2]
 = \operatorname{E}_{x\sim U} [\operatorname{E}[ f|B_i]^2]
$$

(Think of $B_i$ as a partition; for a partition, $\operatorname{E}[f|P]$ is a function of $x$ that takes $x$ to the average value in the atom of the partition that contains $x$.)
%Closer to 1, closer it is to boolean. This is maximized if every bucket/block is either constantly 1 or 0.
Note this have value in $[0,1]$ and is maximized if $f$ is constant on every block. We show every iteration increases this potential 
function by at least a fixed amount, $(\varepsilon\delta)^2$.
%$\Omega((\varepsilon \delta)^2)$. 

Fix a block $B$ in the partition. Define $p,q,\alpha_+,\alpha_-,p_0,p_1$ as follows. 
\begin{align}
p&=\Pr[f=1|B]\\
q&=\Pr[h_{i+1} = 1|B]\\ %split into 2 halves, q overall prob next fun is 1 on block
q+\alpha_+ &= \Pr[h_{i+1}=1|B, f=1]\\ %correlated with $f$. slightly higher
q-\alpha_- &= \Pr[h_{i+1}=1|B, f=0]\\ %cond on 0, slightly lower
%on some blocks it could flip. On average >0
\alpha_+p &= \alpha_-(1-p) \text{ by conservation}\\
%B&\mapsfrom \alpha_-(1-p)\\
%B_i&=B\cap \{h=i\}\\
p_0&= \Pr[f=1|h=0,B] = \frac{\Pr[f=1\wedge h=0|B]}{\Pr[h=0|B]} = \frac{p(1-q-\alpha_+)}{1-q}\\
p_1&= \Pr[f=1|h=1,B] = \frac{\Pr[f=1\wedge h=1|B]}{\Pr[h=1|B]} = \frac{p(q+\alpha_+)}{q}\\
\operatorname{E}_{x\in B} [\operatorname{E}[f|B_{i+1}]^2]&=qp_1^2 + (1-q)p_0^2 = p^2 \left(\frac{(q+\alpha_+)^2}{q} + \frac{(1-q-\alpha_+)^2}{1-q}\right) \\
&=p^2\left(
\left(q+2\alpha_+ + \frac{\alpha_+^2}{q}\right)
+
\left(1-q-2\alpha_+ + \frac{\alpha_+^2}{1-q}\right)\right)
\\
&=p^2\left(1+\frac{\alpha_+^2}{q} + \frac{\alpha_+^2}{1-q}\right)\\
&\ge p^2 + 4p^2 \alpha_+^2 \ge
p^2+
 \alpha_+^2\\
 %multiplicative increase by $1+\alpha$?
% \operatorname{E}_{x\sim U} [
\operatorname{E}[f|B_{i+1}]^2 - \operatorname{E}[f|B_i]^2%]
&=\alpha_+^2(B_i(x)).
\end{align}

%\alpha_+ cancel
%worst case $q=\rc 2$
%if $\operatorname{Maj}=1$.
%retro assume majority block 1

%instead of look at induced prob take exp %measure of $\mu$ times advantage
Assume WLOG that $\operatorname{Maj}(B_i(x))=1$.  (Otherwise the LHS is smaller.)
\begin{align}
\operatorname{E}_{x\in B}[\mu(x) ((-1)^{(h(x)\ne f(x))})]
&=\quad p\left(\frac{1-p}{p}\right) \left[(q+\alpha_+) - (1-q-\alpha_+)\right]&(f=1)\\
&\quad +(1-p) 1 [1-(1-\alpha_-) - (q-\alpha_-)]&(f=0)\\
&=(1-p) (2\alpha_++2\alpha_-)\\
&= 2\alpha_+(1-p) + 2\alpha_+p=2\alpha_+\\
\operatorname{E}_{x\sim U} 2\alpha_+(B_i(x)) 
&=\operatorname{E}_{x\sim U} [\mu(x) ((-1)^{h(x)\ne f(x)})]\\
&\ge \varepsilon|\mu| \ge 2\delta \varepsilon\\
\varphi_{i+1}-\varphi_i &\ge 
\operatorname{E}_{x\sim U} [\operatorname{E}[f|B_{i+1}]^2 - \operatorname{E}[f|B_i]^2]\\
&\ge 
\operatorname{E}_{x\sim U} \alpha_+^2(B_i(x))\ge  (\delta\varepsilon)^2.
\end{align}â€¢
%Expected increase is at least $(\delta\varepsilon)^2$.
%got rid of $p^2$, assume $p\ge \rc 2$.

%Every step don't terminate, potential function increases by $(\delta\varepsilon)^2$. It's a number in 
Because $\varphi_i$ is always in $[0,1]$, the number of iterations is at most $k\le (\delta \varepsilon)^2$.
\end{proof}

\section{Comments, Regularity lemmas}

Some comments:

\begin{enumerate}
\item
All you get from this proof is a decision tree; the complexity is exponential in $k$. This is a bug, not a feature.

In complexity terms, we don't get good hard-core measure, because the circuit size for the outer function $G$ is $2^k$.
A better boosting algorithm would give $G$ have smaller complexity. If your stopping point is the hard-core lemma, this is not the boosting algorithm you want. For the dense model theorem, this is fine because all you care about is size of $k$, not the  complexity of $G$.

There is another boosting algorithm which gives a weighted majority function, which is a simpler function. A weighted majority can be converted into a decision tree, but not vice versa.
\item
This potential function matches this boosting algorithm. Other boosting algorithms can be analyzed with other potential functions. 
%binary entropy

This is like the potential function used most in graph theory. Key property: you can't make negative progress; you always go forwards.
%Translating this to graph theory, you get the usual Szemeredi Theorem except without equal partitions.
\item 
For Szemeredi regularity, we need a stronger boosting theorem. Suppose we get stuck at some step: no function correlates globally, but there are many blocks where we can find functions that correlate with the function inside that block. If in $\varepsilon$ fraction of blocks we find functions that correlate, partition them based on all the values of these functions, and repeat.

In one step we've gone from order of $2^k$ to order of $2^{2^k}$ buckets, and increased the potential function by a polynomial in terms of $\varepsilon,\delta$. This is a familiar argument; we can only go $\frac{1}{\varepsilon}$ iterations before we terminate. This time, the number of sets is a tower depending on $\varepsilon$.
\item
Regularity lemmas: 

Fix a set of vertices $V$ of set $n$. Let $U$ be edges in complete graph on $V$.
(We can also consider the case when $U$ is not the complete graph, ex. $U$ is the edges in $d$-regular expander on $V$.)

The underlying set we care about is the set of cuts defined by $A,B\subseteq V$ where $A\cap B=\phi$; there are $3^k$ of them.

If $|E|\ge \delta \binom n2$, the generic regularity lemma says there exists $\mu=G(T_1,\ldots, T_k)$, where $k=O(1/\varepsilon^2\delta^2)$, that is a good predictor the number of edges of any cut in the graph. 
Use the $T$'s to divide the vertices into $3^k$ subsets such that $\mu$ is a constant on every pair of subsets.
$$
\frac{E_G(A,B)}{|E_G|}
\approx_\varepsilon \sum_{i,j} 
\mu_{ij}
\frac{|A\cap A_i||B\cap B_j|}{|V|^2}.
$$

This is the weak regularity of Frieze-Kannan. For Szemeredi we need the stronger boosting lemma (see previous point).

We can also do something similar with $G$ a subset of an expander. The expander mixing lemma gives an error term.
\end{enumerate}

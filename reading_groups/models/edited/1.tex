\section{Big picture}

We'll talk about several results which have different names in different fields. %guises 
You probably know them, but don't know the same or related idea comes up in the other fields.\\

\begin{tabular}{|p{25mm}|p{25mm}|p{25mm}|p{25mm}|p{25mm}|p{1mm}|}
\hline 
 & Boosting & Hard-core lemma & Dense model theorem & Weak regularity & ?\tabularnewline
\hline 
Area & ML & CC,  Derandom-ization & Additive combinatorics, CC & Graph theory & \tabularnewline
\hline 
Credit & Shapiro, Freund-Schapire & Impagliazzo, Holenstein & Green-Tao, Barak-Shaltiel-Wigderson & Szemeredi, Frieze-Kannan & \tabularnewline
\hline 
Get & Circuit computing $f$ $1-\delta$ of the time & '' & Proof that set isn't $\delta$-dense & '' & \tabularnewline
\hline 
Unless & Weak learner fails on distribution of density $\Omega(\delta)$ & Hard-core distribution  & $\Omega(\de)$-dense ``model'' indistinguishable from set & A model succinctly describing set & \tabularnewline
\hline 
Algorithm needed %to be const.
 & Weak learner & '' & Distinguisher & '' & \tabularnewline
\hline 
\end{tabular}

We will take these theorems that we know to be true and show implications between them. Implications are due to...
\begin{enumerate}
\item
Boosting$\implies$Hard-core: 
Klivans and Servedio.
\item
Hard-core$\implies$Dense model:
Impagliazzo
\item Dense model$\implies$Weak regularity:
Trevisan-Tulsiani-Vadhan, Reingold-Trevisan-Tulsiani-Vadhan
\item Weak regularity$\implies$boosting: Trevisan-Tulsiani-Vadhan
\end{enumerate}•

What can we gain from looking at these connections?
\begin{enumerate}
\item
Versatility: We can ``retrofit'' algorithms for one setting to get algorithms for the other settings.

For example, there are many boosting algorithms. When you follow this progression, you get different quantitative and qualitative versions of dense model theorem and regularity.
\item
Algorithmic and constructive results: 

There are nonconstructive versions using the min-max theorem for boosting, hard-core lemma, dense model theorem. We care about algorithmic versions. 

Note that the algorithmic result that we care about is different in the different settings.  In ML we care about getting a function that computes a function much of the time. On the other side, we're really after the distribution where the weak learner fails, so that we get a model that succinctly describes the set. 

We pay attention to do the reductions in an algorithmic, not just an existential way.
\item
Using the dense model theorem for learning. Can we take a boosting technique and use it in an unsupervised way?
%(The dense model theorem can be thought of as giving a way to represent a dataset succinctly.)
\item
Generality: some things seem to be specific to a setting (density of graphs). 

But actually, weak regularity doesn't have anything to do with graphs being dense. We can relativize it to subgraphs of any graph. You can look at subgraphs of expanders, bipartite graphs, etc., and plug it in the same machinery. Likewise if you want to look at spectral norms rather than cuts.
\end{enumerate}•

%\section{Boosting}

\section{Setup}
First we discuss the PAC learning model.

%Let $U$ be the universe (a big set of possible inputs) and $f:U\to \{0,1\}$ be a boolean function. 
Let $U$ be a set, and by abuse of notation, also a distribution on that set. (Think of $U$ as the universe, the set of possible inputs.) For simplicity, take the distribution to be uniform.
%distribution on a set $X$, and 
Let $f:U\to \{0,1\}$ be a boolean function.
A learning algorithm can request any number of points $(x,f(x))$ where $x\sim U$. The goal is to find a hypothesis $h$ such that 
$$
\Pj_{x\sim U} [h(x)=f(x)]\ge 1-\de.
$$
\begin{thm}
A \vocab{strong learner} for $(U,f)$ with hypothesis class $\mathcal H$ is an algorithm such that given samples $(x,f(x)), x\sim U$, outputs $h\in \mathcal H$ (with high probability) such that 
$$\Pj_{x\sim U}[h(x)=f(x)]\ge 1-\de.$$

(Typically, we say that the probability of success is $1-\ep$, ask for a strong learner for all $f\in \mathcal F$, and require it to run in time $\poly(\rc \ep, \rc \de)$.) 
\end{thm}

In boosting, we assume that we have weak learners.
\begin{thm}
A $\ep$-\vocab{weak learner} for $(\mu, f)$ with hypothesis class $\mathcal H$ is an algorithm such that given $(x,f(x)), x\sim \mu$, outputs $h$ (with high probability) such that
$$
\Pj_{x\sim \mu} [h(x)=f(x)] \ge \rc 2+\ep.
$$
\end{thm}
It only has to output a function that is somewhat correlated with the right answer.
Typically, we ask the weak learner to work on any distribution $\mu$ satisfying some assumptions.

In order to use a weak learner, we construct a routine that subsamples the distribution $U$ to pass to pass to the weak learner.
\begin{df}
Let $\mu:U\to [0,1]$. Define the probability distribution 
$$
D_\mu(x) = \fc{\mu(x)}{\sum_{x'\in U}\mu(x')}.
$$
\footnote{When $U$ is not uniform and has distribution $u(x)$, this is $\fc{\mu(x)u(x)}{\sum_{x'\in U} \mu(x')u(x')}$.}
\end{df}
Think of this as rejection sampling: pick $x\sim U$, keep it with probability in $[0,1]$, or else throw if back and repeat.

In order for this sampling to be efficient, we need $\mu$ to not be too small.
\begin{df}
Define the \vocab{density} of $\mu$ in $U$ to be
$$|\mu| = \EE_{x\in U} \mu(x).$$ 
\end{df}
%Think of this as a notion of density. 

We will use weak learners in the following context. 
\begin{enumerate}
\item
We will only run weak learners on distributions whose density is not too small (the dependence on $\de$ is $|\mu|=\Om(\de)$). We don't want to run a weak learner on a distribution of very low density, because the time to simulate  the distribution is inversely proportional to the density.
\item
We ask the weak learners to output a function in a given class $h\in \mathcal T$. 

Then it will turn out that that both the measures that we run the weak learners on, and the final hypothesis, will be describable using $\mathcal F_l \mathcal T$ (see below), for some class $\mathcal F$.
\end{enumerate}

\begin{df}
Say that a set $\mathcal T$ of functions $U\to \{0,1\}$ form a class if $f\in \mathcal T$ implies $1-f \in \mathcal T$. 

Let $\mathcal F$ be a class of boolean functions. Define the class of functions
$$
\mathcal F_k \mathcal T = \set{f(h_1(x),\ldots, h_k(x))}{f\in\mathcal F, h_1,\ldots, h_k\in \mathcal T}.
$$
\end{df}

\section{Intuitions}

\begin{enumerate}
\item

\end{enumerate}•

\section{Boosting and the Hard-core lemma}

%If you just care about ML we don't care about this details, but to go further, we do.

The first boosting algorithm we give is totally ridiculous from the ML point of view. For people who work on weak regularity on graphs this is the natural version, and leads to the standard versions of results.

We will take $\mathcal F$ to be the set of all boolean functions, so given hypotheses $h_1,\ldots, h_k$, we can choose the best predictor using $h_1(x),\ldots, h_k(x)$.

\begin{thm}[Boosting with decision trees]\label{thm:boosting}
Let $U$ be a distribution, $\mathcal T$ a class of boolean functions $U\to \{0,1\}$,  $\mathcal F$ the class of all boolean functions. 
Let $f:U\to \{0,1\}$ be a given function (which we are trying to learn).

\begin{enumerate}
\item
Suppose that there is a $\de$-weak learner such that given any distribution $\mu$ on $U$ with $|\mu|\ge 2\de$, it produces $h\in \mathcal T$ such that 
$$
\Pj_{x\sim \mu} [h(x) = f(x)] \ge \rc 2+ \ep.
$$
%with hypothesis class $\mathcal H$ 
\item
Then there is a strong learner that produces $h\in \mathcal F_k\mathcal T$ with $k\le\ce{\rc{\ep^2\de^2}}$ such that 
$$
\Pj_{x\sim U} [h(x) = f(x)]\ge 1-\de.
$$
\footnote{
We ignore sample complexity here. In reality, because we only see $U$ from samples, we need to think about generalization. If the VC-dimension of $\mathcal T$ is $d$, then the VC-dimension of $\mathcal F_k\mathcal H$ is at most $k^d$. In ML we don't want to take $\mathcal F$ to be the class of all boolean functions. For this theorem, let's just assume we are actually given all pairs $(x,f(x))$.
}
\end{enumerate}
\end{thm}

\begin{thm}[Hard-core lemma]
\label{thm:hardcore}
Let $U$ be a distribution, $\mathcal T$ a class of boolean functions $U\to \{0,1\}$, $\mathcal F$ the class of all boolean functions.

Then either
\begin{enumerate}
\item %(Function is \fixme{...})
There exists $h\in \mathcal F_k \mathcal T$ such that 
$$
\Pj_{x\sim U} [h(x)=f(x)] \ge 1-\de,
$$
where $k\le\rc{\ep^2\de^2}$,
or
\item (There exists a hard-core distribution.)
There exists $|\mu|\ge 2\de$ on $U$, such that for all $h\in \mathcal T$, 
$$
\Pj_{x\sim \mu}[h(x) = f(x)] \le \rc 2+\ep.
$$
\end{enumerate}
\end{thm}

Note it is important for us to keep track of the size of the hardcore distribution, which is $\ge 2\de$ here.
Different boosting algorithms will give the result for different classes of functions $\mathcal F$.

\begin{proof}[Proof of hard-core lemma~\ref{thm:hardcore} from boosting~\ref{thm:boosting}]
Let weak learner be exhaustive search over $\mathcal T$. The weak learner operates on distributions $|\mu_i|\ge 2\de$. If it always produces $h_i$ with bias $\ge \de$, then continue  and obtain the strong learner: we get some $H\in \mathcal F_k \mathcal T$ such that  $H(x)=f(x)$ with probability $1-\de$. 

If at some step $i$ our exhaustive search algorithm gets stuck, we get a distribution $\mu_i$ that's hard-core.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:boosting}]
%any tests, underlying dist, boosting alg
%applications of strong regularity going other way?
%can use to prove strong reg, but don't know generic
%how to define properties particular algorithm has and see if it's interesting for boosting itself. 
%not so great as a boosting algorithm, although other assm make it feasible

The algorithm is as follows. Let $WL(\mu)$ denote the weak learner operating on $(\mu, f)$.

Let $\mu_0$ be constant 1, $i=0$.

While $|\mu_i|\ge 2\de$, do
\begin{itemize}
\item
$h_{i+1}\mapsfrom WL(\mu_i)$.
\item
Partition $X$ according to values of $h_1,\ldots,h_i$.

Let $h_{1:i}(x):= (h_1(x),\ldots, h_i(x))\in \{0,1\}^i$, and let $B_i(x)$ be the ``block'' that $x$ is in, 
$$B_i(x) = h_{1:i}^{-1}(h_{1:i}(x)) = \set{y\in U}{h_{1:i}(x)=h_{1:i}(y)}.$$ 
%For a function $f$, let $\Maj(f)$ denote the majority value of $f$.
For a set $B$, let $\Maj(B)$ denote the majority value of $f$ on $B$. 
\item
Define $\mu_{i+1}$ by 
%
$$\mu_{i+1}(x) = \begin{cases}
\fc{1-p_{\Maj, B_i(x)}}{p_{\Maj, B_i(x)}},&\text{if } f(x)=\Maj(B_i(x))\\
1,&\text{otherwise}
\end{cases}• $$
%where $p_{\Maj} = \Pj(h_{i+1}(x)= \Maj(h_{i+1}))$.
where $p_{\Maj,B} = \Pj(f(y) = \Maj(B)| y\in B)$, the proportion of the majority in $B$.
\item
$i\mapsfrom i+1$.
\end{itemize}
Finally, return $H_{i}(x) = \Maj({B_{i}(x)})$, i.e., look at the block that $x$ is in, and choose the majority value.

Note that the measure $\mu_{i+1}$ rebalances %so that $\Pj_{\mu_{i+1}}(h_{i+1}(x) = 1)= \Pj_{\mu_{i+1}}(h_{i+1}(x)=0)=\rc2.$
each block $B_i$ such that conditioned on $y$ being in a block $B_i(x)$, 
$$\Pj_{y\sim \mu_{i+1}}(f(y)=1|y\in B_i(x)) = \Pj_{y \sim \mu_{i+1}}(f(y)=0|y\in B_i(x))=\rc 2.$$
Indeed, we have
\begin{align}
\EE_{y\sim U}[\one_{f(y)=1} 
%\sum_{y\in B_i(x), f(y)=1}
 \mu_{i+1}(y) |y\in B_i(x)]
 &=p_{\Maj, B_i(x)} \fc{1-p_{\Maj, B_i(x)}}{p_{\Maj, B_i(x)}} = 1-p_{\Maj, B_i(x)}\\
 \EE_{y\sim U}[\one_{f(y)=0} 
%\sum_{y\in B_i(x), f(y)=1}
 \mu_{i+1}(y) |y\in B_i(x)]
 &=\pa{1-p_{\Maj, B_i(x)}}\cdot 1 = 1-p_{\Maj, B_i(x)}\\
|\mu_{i+1}| =  \EE_{y\sim U}[
 \mu_{i+1}(y)]&=\sum_{\text{block }B_i} [2(1-p_{\Maj, B_i}) \Pj(B_i)]\\
 & \ge 2(1-p_{\Maj,U}).
\end{align}

%Note that $p_{\Maj} \ge 1-\de$ iff for $x$ in the majority, $\mu_{i+1}(x) \le \fc{\de}{1-\de}$. Thus if 
Note that if $|\mu_{i+1}|\le 2\de$, then $\Pj_{x\in X}[H_i=f]\ge 1-\de$, and we are done. (We stop before we have to apply the weak learner to a distribution of density $<\de$.)

We need to show this method terminates in a bounded number of steps.

Consider the potential function
$$
\ph_i = \E_{x\sim U} [(\Pj[f=1|B_i(x)])^2]
 = \E_{x\sim U} [\E[ f|B_i]^2]
$$
(Think of $B_i$ as a partition; for a partition, $\E[f|P]$ is a function of $x$ that takes $x$ to the average value in the atom of the partition that contains $x$.)
%Closer to 1, closer it is to boolean. This is maximized if every bucket/block is either constantly 1 or 0.
Note this have value in $[0,1]$ and is maximized if $f$ is constant on every block. We show every iteration increases this potential 
function by at least a fixed amount, $(\ep\de)^2$.
%$\Om((\ep \de)^2)$. 

Fix a block $B$ in the partition. Define $p,q,\al_+,\al_-,p_0,p_1$ as follows. 
\begin{align}
p&=\Pj[f=1|B]\\
q&=\Pj[h_{i+1} = 1|B]\\ %split into 2 halves, q overall prob next fun is 1 on block
q+\al_+ &= \Pj[h_{i+1}=1|B, f=1]\\ %correlated with $f$. slightly higher
q-\al_- &= \Pj[h_{i+1}=1|B, f=0]\\ %cond on 0, slightly lower
%on some blocks it could flip. On average >0
\al_+p &= \al_-(1-p) \text{ by conservation}\\
%B&\mapsfrom \al_-(1-p)\\
%B_i&=B\cap \{h=i\}\\
p_0&= \Pj[f=1|h=0,B] = \fc{\Pj[f=1\wedge h=0|B]}{\Pj[h=0|B]} = \fc{p(1-q-\al_+)}{1-q}\\
p_1&= \Pj[f=1|h=1,B] = \fc{\Pj[f=1\wedge h=1|B]}{\Pj[h=1|B]} = \fc{p(q+\al_+)}{q}\\
\E_{x\in B} [\E[f|B_{i+1}]^2]&=qp_1^2 + (1-q)p_0^2 = p^2 \pa{\fc{(q+\al_+)^2}{q} + \fc{(1-q-\al_+)^2}{1-q}} \\
&=p^2\pa{
\pa{q+2\al_+ + \fc{\al_+^2}{q}}
+
\pa{1-q-2\al_+ + \fc{\al_+^2}{1-q}}
}
\\
&=p^2\pa{1+\fc{\al_+^2}{q} + \fc{\al_+^2}{1-q}}\\
&\ge p^2 + 4p^2 \al_+^2 \ge
p^2+
 \al_+^2\\
 %multiplicative increase by $1+\al$?
% \EE_{x\sim U} [
\E[f|B_{i+1}]^2 - \E[f|B_i]^2%]
&=\al_+^2(B_i(x)).
\end{align}

%\al_+ cancel
%worst case $q=\rc 2$
%if $\Maj=1$.
%retro assume majority block 1

%instead of look at induced prob take exp %measure of $\mu$ times advantage
Assume WLOG that $\Maj(B_i(x))=1$.  (Otherwise the LHS is smaller.)
\begin{align}
\EE_{x\in B}[\mu(x) ((-1)^{(h(x)\ne f(x))})]
&=\quad p\pf{1-p}{p} \ba{(q+\al_+) - (1-q-\al_+)}&(f=1)\\
&\quad +(1-p) 1 [1-(1-\al_-) - (q-\al_-)]&(f=0)\\
&=(1-p) (2\al_++2\al_-)\\
&= 2\al_+(1-p) + 2\al_+p=2\al_+\\
\EE_{x\sim U} 2\al_+(B_i(x)) 
&=\EE_{x\sim U} [\mu(x) ((-1)^{h(x)\ne f(x)})]\\
&\ge \ep|\mu| \ge 2\de \ep\\
\ph_{i+1}-\ph_i &\ge 
\EE_{x\sim U} [\E[f|B_{i+1}]^2 - \E[f|B_i]^2]\\
&\ge 
\EE_{x\sim U} \al_+^2(B_i(x))\ge  (\de\ep)^2.
\end{align}•
%Expected increase is at least $(\de\ep)^2$.
%got rid of $p^2$, assume $p\ge \rc 2$.

%Every step don't terminate, potential function increases by $(\de\ep)^2$. It's a number in 
Because $\ph_i$ is always in $[0,1]$, the number of iterations is at most $k\le (\de \ep)^2$.
\end{proof}

\section{Dense model theorem}

\begin{df}
For a set $S\subeq U$ and a function $T:U \to \{0,1\}$,  let $T(S):=\E_{x\in S} T(x)$. (For a measure $\mu: U\to [0,1]$, also write $T(\mu) = \E_{x\sim \mu} T(x)$.)

Let $S\subeq U$ be a subset, and let $\mathcal T$ be a set of tests. $S$ is \vocab{$(\ep,\de)$-pseudo-dense against $T$} if for all $T\in \mathcal T$, 
$$
T(U) \ge \de T(S)-\ep.
$$
\end{df}
Think of saying that the tests $\mathcal T$ don't reveal that the set $S$ is small. 
\begin{enumerate}
\item
One way of being pseudo-dense is to actually be dense.
\item
Another, one step removed, is that there's a set $R$ that's indistinguishable from the whole distribution (in the sense of being at least about $\de$ fraction of it), and the set is dense in $R$.
\end{enumerate}•
\begin{df}
For two distributions $\mu_1,\mu_2$ on $U$, we say that $\mu_1,\mu_2$ are indistinguishable by tests in  $\mathcal T$ up to $\ep$, written $\mu_1 \sim_{\mathcal T} \mu_2$ within $\ep$, if for every $T\in \mathcal T$, 
$$
|\E_{\mu_1}T - \E_{\mu_2} T| \le \ep.
$$
\end{df}
\begin{thm}[Dense model theorem]\label{thm:dmt}
Let $\mathcal T$ be a class of tests $U\to \{0,1\}$. 

If $S$ is $(\ep,\de)$-pseudodense against $F_k\mathcal T$, $k=O\prc{\ep^2\de^2}$ then there exists $\mu$, $\mu\in F_k\cal T$ such that $|\mu|\ge \fc{\de}{1+\de}-O(\ep)$ and $D_\mu\sim_{\cal T} S$ to within $O(\ep/\de)$. 
\end{thm}
The idea in the proof is to use the Hard-core lemma, with the hard function being membership in $S$.
\begin{proof}
Let $U'$ be the following distribution: let $\de'=\fc{\de}{1+\de}$ and 
\begin{enumerate}
\item
with probability $\de'$, take $x\in S$ and output $(0,x)$
\item
with probability $1-\de'$, take $x\in U$ and output $(1,x)$.
\end{enumerate}
Define a test $T\in \mathcal T$ to operate on an example $(y, x)$ by $T(y,x)=T(x)$. For $T\in \mathcal F_k \mathcal T$, 
\begin{align}
\Pj_{U'}[T((y,x))=y] = 
\de' T(S) + (1-\de') (1-T(U))
&= 1-\de' + \de' (T(S)) - (1-\de') T(U)\\
&= 1-\de' + \rc{1+\de} (\de T(S) - T(U))\le 1-\de'+\ep.
\end{align}
No test in $\mathcal F_k \mathcal T$ can be correct with probability $>\de'-\ep$. By the Hard-core Lemma~\ref{thm:hardcore}, there exists $|\mu'|\ge 2(\de'-\ep)$ such that for any $T\in \mathcal T$, $\Pj_{(x,y) \sim U'}[T(x)=y]\le  \rc2+\ep$.

In order for $\mu'$ to be hardcore, it must be split approximately evenly between $U$ and $S$ (up to $\ep$); otherwise; we could have an advantage by predicting constant 0 or 1. 
Thus each part has at least $2(\de'-\ep) \pa{\rc 2 - \ep} = \de'\pa{1-O\pf{\ep}{\de}}$ of the mass. Then
$$D_{\mu'|_U}\sim_{O(\ep)} D_{\mu'|_S}\sim_{O\pf{\ep}{\de}} S.$$
%. Now 
%$$\EE_{(y,x)\sim U'} \one_{S}(x)\mu'(x) \ge 2(\de'-\ep)(\rc 2- \ep') \ge \de\pa{1-O(\de)-O\pf{\ep}{\de}}$$ 
%so $D_{\mu'|_S}\sim_{O\pf{\ep}{\de}} S$. 
%Putting this together gives $D_{\mu'|_U} \sim_{O\pf{\ep}{\de}} S$.
\end{proof}







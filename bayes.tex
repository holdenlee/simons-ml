\section{Nonparametric Bayesian methods, Tamara Broderick and Michael Jordan}

This is different than many other ML topics. There is a lot of theory but much of it is open: not clear what the problems are.

Theorems rate, limiting distribution. Bernstein-von Mises, stronger than optimization where you don't know the limiting coefficients.

But that's the statistical; theory. The algorithms here are not the t. math structure that leads to certin classes of models. Think of components of model, put together with prof. Stochastic processes, links to combi, linear algebra, analysis.

Levy-Kitchine. 

Linear functional plus regularization
%theoretician, optimization problems, asyn dist accelerated


Then apply standard algorithms

Combinatorial problems. 
%more from cs view
richer than Gaussian models

not algorithm, analysis of algorithm. Combo, stoch processes, random measure.

What does nonparametric Bayes mean?
\begin{enumerate}
\item
Bayes's Rule says
$$
\Pj\pat{parameters|data} \propto \Pj\pat{data|parameters} \Pj\pat{parameters}
$$
We want to understand the generative model for the data so we can apply Bayes's Theorem.

\item Nonparametric: %not tfinite param unbounding/growing/infinite number of param
not saying we don't have parameters, but have growing number of parameters, infinite latent parameter

%``A long trek home'': start from Seattle, hike, etc. to Alaska.
\begin{itemize}
\item
We can read article after article on Wikipedia. No matter how many you've read there's always more topics to explore. We may be under the belief that no matter how many we've read, there are more. 
\item
Species discovery. In the past week, a bunch of new species were discovered.
%flapjack octopus
\item
Exercises
\item As you analyze a social networks you find more friend groups.
\item
Loooking images, you find more unique objects.
\item
More ancestral groups.
\item
More health issues as you examine more of population
\item
Density estimation from more data points. %Something that requires more nance.
\end{itemize}


\end{enumerate}•
We want more parameters as we examine more data to express more nuance.

I'll focus on a particular model, clustering. The  NPBayes model is the Dirichlet process.

Big questions:
\begin{itemize}
\item
Why NPBayes?
\item
What does a growing/infinite number of parameters really mean (in NPBayes)?
%structural risk min
\item
Why is NPBayes challenging but practical?
\end{itemize}
Note this is a stationary model that allows 
%Selecting a model vs. averaging over model of all cardinalities.

\subsection{Clustering}

In a finite Gaussian mixture model with $K=2$ clusters we have parameters
\begin{itemize}
\item
means $\mu_k\sim N(\mu_0,\Si_0)$ iid
\item
Proportions
$\rh_1\sim Beta(a_1,a_2)$, $\rh_2=1-\rh_1$.
This is conjugate with respect to the categorical distribution. We make convenient distribution. (In research we do want to ask whether these are appropriate.)
\item
Assignments
$z_n\sim Categorical(\rh_1,\rh_2)$
\item
The data are generated by 
$$
x_n\sim N(\mu_{z_n},\Si).
$$
\end{itemize}•
(When do you stop putting priors on things? We assume $\mu_0$ is known.)

The beta distribution is
$$
Beta(\rh_1|a_1,a_2) = \fc{\Ga(a_1+a_2)}{\Ga(a_1)\Ga(a_2)} \rh_1^{a_1-1} (1-\rh_1)^{a_2-1}.
$$
Intuition: 
\begin{itemize}
\item
For $(\rh_1,\rh_2)\sim Beta(a,a)$ for $a$ small, we get extremes: most of the mass is on one or the other.
\item
For $a$ large, we get more even distributions.
\item
For $Beta(a_1,a_2)$, $a_1<a_2$, $\rh_2$ has more mass. (On average, $\fc{a_2}{a_1+a_2}$.)
\end{itemize}
For $k$ clusters, 
\begin{itemize}
\item
means $\mu_k\sim N(\mu_0,\Si_0)$ iid
\item
Proportions
$\rh\sim Dirichlet(a_{1:K})$. This is the generalization of the beta distribution.
\item
Assignments
$z\sim Categorical(\rh_{1:K})$
\item
The data are generated by 
$$
x_n\sim N(\mu_{z_n},\Si).
$$
\end{itemize}
The Dirichlet distribution is
$$
Dirichlet(\rh_{1:K}, a_{1:K}) = \fc{\Ga(\sum kK a_k)}{\prodo kK \Ga(a_k)}\sumo kK \rh_k^{a_k-1}.
$$
Dirichlet(1,1,1,1) is uniform over the probability simplex. $\rh_1$ is Beta(1,3). 
\begin{itemize}
\item
If all $a$ are small, most mass is on one of the $\rh$'s.
\item
If all $a$ are large, mass is approximately evenly distributed.
\end{itemize}•
So far $K\ll N$. 
We assumed we were seeing all the clusters.
What if this is not true? 
%$K$ is fixeds. 

The frequentist view: Given clustering, I'm trying to find model. Bayes: Given model, what do data look like. In Bayes inf, let's make a story about how to generate data given parameters. Bayes's Theorem turns that around, probability of parameters given data. %We're exploring the story, assumptions. %If we have the wrong assumptions, we'll capture 
Think species sampling. Go to island, don't know about its biology, start collecting organisms (cf. Good-Turing).
There are many species we haven't sampled yet.
Similar with topic modeling, we don't think we've found them all. There are probably empty clusters, clusters that have no observed data points.

We make a distinction between components and clusters---number of components actually represented in the data.

%\rh_1 to $\rh_{1000}$. Some $\rh$s are so tiny we're not seeing them. We're getting 1000 probabilities.
Ex. consider $K=1000$. Draw $\rh$ from the Dirichlet distribution, represent it as a partition of $[0,1]$ ($0, \rh_1, \rh_1+\rh_2,\ldots$).
%when choose component, probability $\rh_{800}$
%prob fall into component is $\rh_{800}$.
%equivalent
%probability mass fill quickly.
%Am I close to seeing all 
The number of component seen after $n$ draws is approximate $\ln n$.

Order clusters by order of appearance. First component we see is the first cluster.  Second point either joins the first cluster or starts the second cluster.
The number of clusters realized is the maximum cluster number.
%not coupon collecting because not uniform.

The number of clusters is random. It grows with the size of the data but stops at $K$. Sometimes this is fine, but often it is difficult to choose a finite $K$ in advance. %difficult to infer %it is difficult to infer, streaming data.
We run the risk of being wrong, running up against $K$, or having $K$ too large and having too many parameters.
Choosing a particular $K$ can be problematic. We will choose $K=\iy$, but how can we represent this? We only represent what we need. 
%truncation analysis

Let's choose $K=\iy$. Does this recover the right $K$? If you have a finite $K$, the prior goes to $\iy$. But in the posterior you can concentrate at that $K$. There are conditions where you get consistency.
%See Miller-

We're doing something that is stationary. The model doesn't change in time. Cf. coalesence in genetics.

At first glance, $K=\iy$ seems like a bad idea. Can we even do this? What are the properties of this model? Can we actually do inference on this? What are the properties of this model? How to generate $K=\iy$ strictly positive frequencies that sum to 1? 

Observe that to generate $\rh_{1:K}\sim Dirichlet(a_{1:K})$, we can generate with a sequence of beta draws,
$$
\rho_1 \sim Beta(a_1,\sumo kK a_k -a_1) \perp 
\fc{(\rh_2,\ldots, \rh_K)}{1-\rh_1} \sim Dirichlet (a_1,\ldots, a_K).
$$
This gives a recursion. This viewpoint is called ``stick-breaking.''
%V_1\sim Beta(a_1,a_2+a_3+a_4)...

To get an infinity of variables summing to 1, we can just not stop!
\begin{align}
V_1&\sim Beta(a_1,b_1)& \rh_1&=V_1\\
V_2&\sim Beta(a_2,b_2)& \rh_2&=(1-V_1)V_2\\
\ldots\\
V_k &\sim Beta(a_k,b_k) & \rh_k &=\ba{ \prodo j{k-1}(1-V_j)}V_k.
\end{align}•
For simplicity, we take $a_k=1,b_k=\al>0$, the Dirichlet process stick-breaking, a.k.a. the Griffiths-Engen-McCloskey (GEM) distribution. (You can show w.p. 1 the $\rh$'s will sum to 1.)

(This is a rich math tool. %Model some phenomenon in reality.
but why is this a good model for reality? We want a growing number of clusters. Cf. Day 1 of statistics: Gaussian. Our model gives heavy tails, not power laws. To get power laws do something more. You need a math framework to start. The first step is a baby step.)

%mixtures
%rich there. 
You can a distribution on $\al$'s, etc. Where do you put your energy in richness. A  lot are nuisance parameters. 

GEM comes out of genetics. 

GEM($\al$): For $\al$ smaller, we get more unequal sizes.

Note we lose the order. Integrate out via the Chinese Restaurant Process. It's a size-biased ordering, order based on order of appearance.

Sort: Poisson-Dirichlet distribution. Given the distribution, sample from them. Now you get something that tends to go down. Somewhere there is a permutation that is factored out.
%de Finetti

The Dirichlet process mixture model:
\begin{enumerate}
\item
$\rh=(\rh_1,\rh_2,\ldots)\sim GEM(\al)$.
\item
$\mu_k \sim N(\mu_0,\Si_0)$, $k=1,2,\ldots$.
%get rid of some of ordering issues. 
%any region has linear independence properties.
Equivalently, take $\rh_i$ mass on $\mu_i$:
$$
G=\sumo k{\iy} \rh_k \de_{\mu_k} \stackrel d= DP(\al, N(\mu_0,\Si_0)).
$$
This is what's called the Dirichlet process.
\item
$z_n \sim Categorical(\rh)$ iid
\item
$\mu_n^* = \mu_{z_n}$, i.e., $\mu_n^*\sim G$.
\end{enumerate}

How do we make draws? On-demand. Draw a random point in $[0,1]$, then keep drawing $\rh$'s and stop when we're past that point. For each $\rh$ drawn, draw the corresponding mean. Draw the datapoint around the cluster to which it's assigned. Everything is finite because it's on demand.

Answers to questions:

\begin{itemize}
\item
Why NPBayes? Learn more from more data.
\item
What does a growing/infinite number of parameters really mean (in NPBayes)?
Components vs. clusters; latent vs. realized.
%structural risk min
\item
Why is NPBayes challenging but practical?
Infinite-dimensional parameter, but finitely many realized.
\end{itemize}•
Typical approaches are to integrate out the infinite parameter or truncate the infinite parameter.

\subsection{Chinese restaurant process}

\footnote{In Brownian motion, the times of the maximum excursion points are GEM. It's a deep mathematical theory.}

Consider the distribution
$$
G=\sumo k{\iy} \pi_k \de_{\phi_k}.
$$
Here $\pi\sim GEM(\al)$ is a sequence of scalars that sums to 1 with probability 1. 
%Keep chewing away the stick. 
Here $\phi_k\sim G_0$ are drawn iid from some base distribution, for example, Gaussians.
%geometrically weights
It could be any distribution; for example, $G_0$ could be a distribution from a function space, or any separable space $\Te$ (Hilbert space, Banach space).
 %, so we get a basis expansion. %Hilbert, Banach space. Only need to be separable.
We say that $G\sim DP(\al,G_0)$.

The height  $\pi$ is given by a separate process independent of $G_0$. A draw $G$ looks like a picket fence, and is an infinite object. $G$ is a random measure; given a set $A$ we can talk about $G(A)$:
$$
G(A) = \sum_k \pi_k \de_{\phi_k}(A) = \sum_{k:\phi_k\in A}\pi_k.
$$
 %which is somewhat correlated with $G_0$. 
cf. A Ising model is a fixed graph of fixed weights. If we have a distribution on the weights, it's the spin-glass model.

$G$ is a stochastic process, an indexed collection $(X_t)_t$ of random variables. %At each time $t$, 
%I claim that $G$ is a stochastic process. 
For a fixed $A$, $G(A)$ is a random variable. Drawing $G$ again, we get different values of $G(A)$.

$\set{G(A)}{A\in \cal A}$ is the collection of random variables, $\cal A$, the set of measurable sets, is the index set.
%The index set is
%
%came out of genetics.
%first studied in 70's.

See Jim Pitman, St. Flour lecture notes, which covers the probability and combinatorics.

%is this the minimal representation?
%fin dist, kolmogorov criteria
%Is this stationary process?

Using the formalism of stochastic processes, we can talk about algebra of stochastic proceesses, hierarchies of stochastic processes, marginalization (ex. we can try to infer whether two data points are in the same cluster).
In Bayes's Theorem $$\Pj(G|X)\propto \Pj(X|G)\Pj(G).$$
$G$ is a stochastic process.
%Combinatorics makes amazing things happen.


Now given $G\sim DP(\al,G_0)$, draw 
\begin{align}
\te_i|G&\sim G\text{ iid}
%run gibbs, slice, hamiltonian mc
\end{align}•
in the classical Bayesian way.
We can instantiate a finite number of $\phi_k$'s, or integrate out (Chinese restaurant process), or slice sampling, which adaptively adjusts the truncation which samples exactly from the posterior.
There are other rich representations, marginalization and augmentation.

We can run Gibbs sampling, etc, and get $k$-means algorithm. (Cf. EM vs. annealing. Initialization for Bayesian things.)
 Spectral algorithms occur at the level $G$. At a higher level, the completely random measure level, you can do other things.

%DP-means
%beyond densities. atom
(The model is too expressive, gives all levy processes on separable spaces.
%too expressive.
We carve it down to make it less expressive and more computationally tractable.)

The Chinese restaurant process gives a distribution on partitions $\pi_{[N]}$ of $[N]$. (Think of $\pi_{[N]}$ as a set of subsets.)
%
%Write $C\in \pi_{[N]}
How do we specify the probabilities?
Define the Chinese restaurant. There are an infinite number of tables. 
The first customer sits at the first table. The second person joins the first table with probability $\fc{1}{1+\al}$ or starts a new one. There is preferential attachment: 
$$
\Pj\pat{customer $n+1$ joins table $c$} = 
\begin{cases}
\fc{|c|}{\al + n},&\text{if }c\in \pi_{[n]}\\
\fc{\al}{\al + n}, &\text{otherwise}.
\end{cases}•
$$

Let's talk about the closely related model, the P\'olya urn. When you draw a ball, put the ball  back with another ball of the same color. Consider a variation with a designated black ball: if you draw black, generate a new color and put it in. (This is also called the Hockey urn.) This is exactly the same as the Chinese restaurant process (each table is a color).

What is the probability after 6 people have come in we have the following partition?
$$
\Pj(\{1,2,5\}, \{3,4\}, \{6\}) = 
\fc{\al}{\al}\prc{\al + 1} \pf{\al}{\al+2}\prc{\al+3} \pf{2}{\al+ 4} \pf{\al}{\al+5}
$$
A critical fact is that this is exchangeable, invariant under permutation. EPPF: exchangeable partition probability function. (Kingman, 60's characterized urn models with EPPF.) %feature models
$$
\Pj(\pi_{[N]}) = \fc{\al^K}{\al^{\ol N}} \prod_{c\in \pi_{[N]}} (|C|-1)!
$$
where $\al^{\ol N} = \al(\al+1)\cdots (\al + N-1)$ is the rising power.
there are $K$ moments where a new table was started, giving $\al^K$. The denominators always have product $\al^{\ol N}$. For the product, given a table, look at the 2nd to $|C|$th person joining the table. The ordering is gone.

There is more general exchangability on groups, involing representation theory.
%fourier theory on groups

%Levy-Khitchine Theorem

\begin{thm}[De Finetti]
Let $(\te_1,\te_2,\ldots)$ be random variables. Suppose that scrambling them gives the same probability: For any permutation $\pi$, $A_k\in \Te$,
$$
\Pj(\te_1\in A_1,\ldots, \te_N\in A_N)
= \Pj(\te_{\pi(1)}\in A_1,\ldots, \te_{\pi(N)}\in A_N,
$$
%$\Pj(\pi(\te)) = \Pj(\te)$. 
%theory in ML ... iid...
then 
there exists a probability measure  $\la$ on the $G$'s such that $$
\Pj(\te_1\in A_1,\ldots, \te_N\in A_N)
\int \prodo in G(A_i)\Pj(\la G).
$$
The converse also holds.
\end{thm}
This is often called the fundamental theorem of Bayesian analysis. There is $G$ so that once you draw $G$, the $A_i$'s are independent.

A sequence of 1's and 0's where someone didn't tell you the probability is exchangeable. Here $\prodo in G(A_i)$ would be the Bernoullis, $P(\la G)$ is the distribution on the Bernoullis. Choosing Beta gives a particular one. %We have a language to describe 
For iid, $\Pj(\la G)$ is a delta function.

If you have the representation, then you have exchangeability because it is invariant under permutation. The other direction is  nontrivial.
If you restrict to Euclidean spaces this is false.

Polya urn is exchangeable in the De Finetti sense. %Pick a color (green), put a green ball back in. Pick another color 
%Pick $n$ colors. The probability that it lies in a . There is an underlying distribution that the balls were picked from. 

Now given $G$, pick $\te_i|G \sim G$ iid. This follows the De Finetti theorem recipe with 
%david blackwell proved for Polya urns %ifrst ball red.
%polya 01 
 $G=\sumo k\iy \pi_k \de_{\phi_k}$. 

%What does De
%hill climbing in probabilities.
Take ratio of probabilities. Start with arbitrary allocation of people. Take Peter and pull him out. Should I put him at the same or not? We can assume he was the last person to arrive. %This is with prior with the actual parameters, a point came from a table with that parameter.
%log growing number of parameters

Why logarithm? The expectation of indicator variables is
$$
\E \one= \sumo nN \fc{\al}{\al+n}\sim \al\ln N.
$$
How is exchangeability/de Finetti useful algorithmically? 
Each data point has its own parameters. Pull someone out, look at other parameters. Sample from an existing group or put it in another. This gives a Gibbs sampler!

How heavy is the tail? Not too heavy. If I want to get power laws, use the extension that generates them, the Pitman-Yor model. There is a discount parameter. Consider the Chinese restaurant process, but with discount parameter $\ga$,
$$
\Pj\pat{customer $n+1$ joins $c|\pi_{[n+1]}$}
 = \begin{cases}
\fc{|C|-\ga}{\al+n},&C\in \pi_{[n]}\\
\fc{\al + \ga k_n}{\al+n}, &\text{otherwise}
\end{cases}
$$
This gets more tables occupied. The $G$ will not be the GEM but something else.

$$\text{CRM}\xra{\text{augmentation}}C\xra{\text{marginalization}}\text{CRP(Polya)}.$$ The CRM is the zoo that generates the $C$'s.

We can prove statistical guarantees of algorithms, not CS-type theorems.

Topic models: not parametric LDA, better way with these tools, nothing known theoretically.

Math art in constructing $G$. Beta has 2 parameters. $Beta(\al_1,\al_2)$ can give Pitman-Yor. Still in stick-breaking family.

%preferential attachment?
%Gibbs type
Broader class of EPPFs are Gibbs type pdfs.
%Permits MCMC

I don't want to assume iid, I assume exchangability. This is more realistic.

%permutation group. invariant to other orbits. You get a parallel theorem.

\subsection{Going back}
Can we go the other way: if we start with the P\'olya urn, can we go back to the stick-breaking process? Consider a special case of the P\'olya urn where you have binarize the variables: 1 means you sit at the first table, 0 means you sit at any other table.
Consider the probability everyone sits at the first table, 
$$
\Pj(Z_1=1,\ldots, Z_N=1) = \fc{1}{\al+1} \fc{2}{\al+2} \cdots \fc{N}{\al + N} = \fc{\Ga(N+1)}{(\al+1)^{\ol N}}.
$$
This looks like a moment. Let's return to De Finetti. Guess the distribution that recovers the moments. Guess the beta distribution. 
We have
$$
\int \eta^N \rc{B(1,\al)} (1-\eta)^{\al - 1}\,d\eta = 
 \fc{\Ga(N+1)}{(\al+1)^{\ol N}}
 =\Pj(Z_1=1,\ldots, Z_N=1) 
$$
%part guess for de Finetti mixture distribution is.
%ratio of gammas
This is beta-Bernoulli, Bernoulli mixed by beta. %We get heavier tails. 
(It has heavier tails.)

This is the $N$th moment of the random variable. 

The probability of sitting at the first table or not is given by $B(1,\al)$. Now let's forget about the people who sat at the first table. Take the 2nd table, look at everyone who came after the first person sitting there. Again it's $B(1,\al)$. %size-biased, ordering.
This breaks off another part of the stick.

Let $\Te$ be the underlying space. Take a partition $A_1,\ldots, A_n$. Put a random measure on it. Get a random variable associated with each region. Get $$(G(A_1),\ldots, G(A_k)).$$ This random vector has a distribution. They add up to 1 and is nonnegative. It comes from a Dirichlet distribution
$$
(G(A_1),\ldots, G(A_k)) \sim Dir(\al G_0(A_1),\al G_0(A_2),\ldots, \al G_0(A_k)).
$$
%Marginal probabilities under the Dirichlet process. 
Dirichlet process has Dirichlet marginals.

We'll prove this as a consequence of what we did. This was taken as the definition in the first paper; they used the Kolmogorov Theorem to define the Dirichlet process as the process that had these marginals, so the process is defined existentially. (This isn't actually quite right: Kolmogorov requires  consistent finite-dimensional distributions plus a topological constraint, which doesn't quite work out.)

%zoo of combinatorics. 
%Why do 
We will define a Gamma process.
%
%A gamma variable goes up and then decays. 
If we have $k$ gamma variables, divide and normalize to get the Dirichlet distribution. This is one definition of Dirichlet.
We define a gamma process which has gamma marginals. 

We define a completely random measure. In general $G(A_i), G(A_j)$ are dependent. (Everything sums to 1 so there is a weak negative correlation.)'Let's define a stochastic process such that $G(A_i),G(A_j)$ are independent. If we have independent we can divide and conquer. 
\begin{df}
A \vocab{completely random measure} is such that 
for any partition $\bigsqcup A_i$, $G(A_i)$ are independent.
\end{df}

A Poisson point process is a set-valued stochastic process, such that the number of points in a set is a Poisson random variable, $Pois(\mu(A))$. Pick a total number of points, then throw at random number in the space. A nonhomogeneous Poisson process has $\mu$ nonuniform, $Pois\pa{\int_A \mu(dx)}$.

%a realization is a random measure.
We can convert a measure into a set-valued function by putting delta functions at each point. 
% It's a measure instead of a set-valued function.
This gives $G(A) = \sumz k{\iy} \de_{\phi_k}(A)$. A Poisson random measure is a completely random measure.

Recommended: E. C.inlar is the Poisson-focused probability book.

How do we get beyond the Poisson? We have an amazing construction due to Kingman.

Take $N=\sumz k\iy \de_{\phi_k}$ where $N(A) \sim Pois(\mu(A))$. Let $\Te \ot \R^+$. Put a non-homogeneous Poisson process on the product space, $\mu = G_0\ot \nu$, $\mu(A\ot E) = G_0(A) \nu(E)$. 

From this Poisson random measure, we get  points $\{(\phi_k,w_k)\}$. One defines location, the other defines height.
$$
G=\sumo k\iy w_k \de_k.
$$
%random in graphs, power laws in graphs.
In CRP you sit at one table. What if tables are not categories but features, so you want to be able to sit at multiple tables? This is a feature model. Dirichlet doesn't deliver that. The beta process does.

%%toss coin to get 1s and 0's. Bit 
%%Beta-Bernoulli in process world.
%$\Te$ are attributes. %1 or 0. %speak Italian, like Chinese food...
%Bit vector is infinite-dimensional, but has a finite number of 1's (a sparse dictionary in Bayesian world).
%Is this a completely random measure?  Coming from Poisson process.
%%strip
%%Have Italian feature. Make 
%%book vector 
%%parameters 
%%This gives 

This is the only way to get completely random measures. %Levy measure is $\nu$.

Cayman: thin book on Poisson process, weekend read. There are many characterization. The most beautiful is the most spare one.

%Levy at Berkeley. 
Levy processes are Brownian motion plus jumps. If we are in 1 dimension, these would be Levy processes.

\begin{df}
The \vocab{gamma process} $G\sim GaP(\al, \be)$ is the completely random measure 
$$\mu(d\phi, d\om) = G_0(d\phi) \al w^{-1} e^{-\be w}\,dw.$$
\end{df}
This integrates to $\iy$, which is what you want. If it was finite, the number of atoms would be finite; I want infinite sums; I want some part of the mass to generate an infinite number of atoms. 
%$\int f(x) P(dx)

%Marginals independently gamma.
We show
$G(A_i)\sim Gam(\al_i,\be_i)$ are independently gamma, so %If independent, then independently gamma, 
we can normalize to get Dirichlet process.
How do you show something has gamma marginals?
I show that the moments match up. Use the moment generating function to do it all at once.

How do you calculate mgf of stochastic processes? That is the Levy-Khintchine Theorem.
Probabilists use this all the time.

The gamma distribution is
$$Gamma(a,b) = \fc{b}{\Ga(a)}z^{a-1} e^{-bz}.$$
It has $\E X = \fc ab$
%all points fall in set $A$.
We have
\begin{align}
G(A) & = \sum_k w_k \de_{\phi_k}(A)\\
&=\int w \one_A(\phi) N(d\phi,dw)\\
&= \int f(w,\phi) N(d\phi, dw).
\end{align}
Integral a function against a random measure, get a number for each $N$, But $N$ is random so this is a random variable.

%MCT
For $\xi = (\phi, w)$, we want $\int f(\xi) N(d\xi)$. Consider first 
\begin{align}
f(\xi) & = c\one (\xi)\\
%measure of c
%integral indicator is probability
\E(\cdot) &= \E\int c \one_C(\xi) N(\xi)\\
&= c\E N(c)\\
&=c \mu(C).
\end{align}
For sums of indicators
\begin{align}
f(\xi) &= \sum c_j \one_{C_j} (\xi)\\
\E (\cdot) &= \sum_j c_j  \one(\xi) N(\xi)\\
&=\sum_j c_j \E N(c_j)\\
&=\sum)j c_j \mu(C_j)\\
&=\int f(\xi)\mu(\xi).
\end{align}
We get this is true for arbitrary $f$ by monotone convergence theorem.

We took the integral  of a deterministic function against stochastic process, $\int f(\xi) N(d\xi)$. The end result is replacing $N$ with $\mu$. 


Levy-Khintchine tells us how to get mgf for stochastic processes. Similar to the above, we get the following.
\begin{thm}[Levy-Khintchine]
$$
\E e^{-t\int f(\xi) N(d\xi)} = 
\exp\pa{-\int (1-e^{-tf(\xi)})\mu(d\xi)}.
$$
\end{thm}
%Random variable into Laplace transform
This is a Lapalce transform. Why do we get $1-e^{-t f(\xi)}$? It's the mgf of Poisson.  %It has a distribution that is Poisson. Y
You're integrating the mgf of a Poisson.

Let's return to the gamma process.
$$
\E e^{-t G(A)} = \E e^{-\int_\Te \int_0^\iy (1-e^{-tw \one_A(\phi)}) G_0(d\phi) \nu(dw)}
$$
where $\nu(d\om)  = \al w^{-1} e^{-\be w}dw$.
%theory about these integrals.
This gives
$$
 = \exp(-\al G_0(A)\int_0^\iy (1-e^{-t\om}) w^{-1} e^{-\be a}\,dw) = \pf{\be}{\be+ t}^{\al G_0(A)}.
$$
%ga mgf
which is the gamma mgf.
%\pf{b}{t+b}^a.

We can design other stochastic processes by  replacing $\nu(d\om)$ with something else like $w^{-1}(1-w)^{a-1}\,dw$.

%exchangeability.

%nonparam hierarchies. How much flexibility is lost by not going back. When stop changing your result is done.

Didn't get to cover: Hierarchical Dirichlet processes, etc. $G_0$ is from a Dirichlet process. Multiple clustering: each document cluster words, but we want to unify  documents among corpus. 
Prove things like: rate is $\ln\ln n$. 

Book: %Art
Aad van der Vaart, Holland: Theory of Bayesian nonparametrics. %bernstein-von Mise
\url{http://www.math.leidenuniv.nl/~avdvaart/BNP/BNP.pdf}

%integral over convex sum of things. extreme points in geometry. convex analysis in space.
%clear that you should choose func form...
%do math that seems broadly useful. Users put into model. 

%entropy, log-sobolev, get upper bound on mgf, try to upper bound
Tamara: conjugacy theory for exponential family-based random measures.

\subsection{Applications}

\url{http://www.tamarabroderick.com}

What do we do with these models besides clustering?
One application: we have different datasets, we share power among them. %We have some apply tree information. Coagulate and fragment.

In probabilistic models for graphs, we capture rich relationships, coherent uncertainties, and prior information.

Example models are stochastic block model, mixed membership stochastic block model, infinite relational models, and many more (Lloyd 2012). 

How do these models capture streaming data?

Are we capturing the right growth properties in these graphs?

One simple property is \vocab{projectivity}: adding more data doesn't change distribution of earlier data.  Then these models and many more are misspecified because they are too dense. (The number of edges as a function of the number of nodes it quadratic, too many. There are many real-life models where we don't see this.)

We give a new framework for sparse graphs. This is a Bayesian nonparametric framework.

%The problem with the SBM: if we assume this is . 
We will come back to clustering and get some characterization theorems.

There is also concurrent and independent work by Crane and Dempsey.

Consider a sequence of graphs. At each step we add some nodes and edges, but never subtract anything.
Imagine this sequence has countably infinite steps.

A particular interesting case is the number of nodes going to $\iy$. A graph sequence is \vocab{dense} if 
$$\# E(G_n) \ge c\# V(G_n)^2.$$
%We have some fraction of a complete graph.
It is \vocab{sparse} if 
$$\# E(G_n) \in o(\# V(G_n)^2).$$
All the graph models we saw before can't satisfy this. (why not?) 
%broad set...
%There is a connection to graphons.

Add one node at each step; label by the step where it comes in. Whenever a node comes in, it connects to all edges it will ever connect to.
Assume we have some distribution over this graph sequence. 
A key property is exchangeability. We don't have data points. 

%graph iso!
The probability doesn't change if we label the nodes differently. (Hoover 1979) We call this node exchangeability. How might we generate an exchangeable distribution for graphs.

Consider a function  $W:[0,1]^2 \to [0,1]$ that is symmetric about $x=y$ (a graphon). %gaussian process
Generate a graph as follows. For each node choose a number $x_i$ in $[0,1]$. %The intersection of the horizontal and vertical lines $
For $(i,j)$ put an edge with probability $W(x_i,x_j)$. This generates a node-exchangeable graph.

\begin{thm}[Aldous, Hoover]
Every node-exchangeable graph has a graphon representation.
\end{thm}
We have
$$
\E[\# E(G_n)] = \E \ba{\binom n2 \rc 2\int_0^1\int_0^1 W(x,y)\,dx\,dy} \sim cn^2.
$$
%Remembering $n$ is the number of nodes, 
%W.p. 1 the graph has quadratic grown. 
Every node-exchangeable graph sequence is dense (or empty) almost surely.

Intuition: to a given node, all other nodes look the same.
%corrections change distribution
See survey by Orbanz, Roy 2015. 

Here is an alternative idea. Label the edges at the step it's added. Bring in a node when it's part of an edge that's been instantiated. This is like emails (sender and receiver). Exchangeability: The probability doesn't change if we change the order of the edges.
 (Ex. the order of the emails doesn't matter.) 

%clusters could change over time.
\begin{thm}[CCB]
A wide class of edge-exchangeable graph models yields sparse graph sequences.
\end{thm}
Our goal:
\begin{enumerate}
\item
characterization theorem for edge-exchangeable graphs.
\item
sparsity theorem for edge-exchangeable graphs.
\end{enumerate}

Along the way we use examples from clustering.

We want clusters to be meaningful (cat, dog, mouse pictures, etc.). We could represent this in a matrix-like form. There is no ordering on columns.
There is something missing in this representation: what if some pictures have multiple animals? We want feature allocation: each datapoint can belong to multiple groups (feature). We have the same exchangeability assumption. %Feature is the group of datapoints that go together.

Graphs are a specific subclass: each data point belongs to 2 columns. Columns are labels. The groups are the vertices. Exchangeability of rows is not edge exchangeability. %We can generate the graph
We allow multigraphs.

This looks like clustering. 
%graphs are feature allocation. 

An exchangeable probability function for a clustering is a symmetric function of the size of the clusters
$$p(S_{N,1},\ldots, S_{N,K}).$$
This is the exchangeable partition probability function.
Pitman showed every exchangeable clustering has an EPPF.
%Unordered set of number of inputs

For feature allocation, making $S_{N,K}$ the size of (number of datapoints in) the $K$th feature, I can define the exchangeable feature probability function (EFPF)
$$
p(N;S_{N,1},\ldots, S_{N,K}).
$$
%BPJ
Many but not all exchangeable feature allocations have an exchangeable feature probability function. 

Suppose each row is an edge, each vertex is a column. $S_{N,K}$ is the degree of the $K$th vertex.  %The number of edges  is a 
%symmetric, don't need $N$.
This is an exchangeable vertex probability function.

Does every edge-exchangeable graph have an EVPF? No, here is a counterexample: Suppose we have 4 vertices. Every time, choose iid an edge 12, 23, 34, or 41. If I had an EVPP, then 12, 34 and 23, 41 would have the same probability. But the probabilities are $p_1p_2$ and $p_3p_4$. 
This is also a proof that we don't always have EFPR's.
%Not every excha feature alloc has EFPF.

%We know more things about 
%EEG

Here is a way to generate exchangeable clusters. Randomly paritition $[0,1]$. For each data point, it belongs to the cluster corresponding to the interval it falls in. When fall in same interval, fall in the same cluster.

\begin{thm}[Kingman]
A clustering is exchangeable iff it has a ``Kingman paintbox'' representation.
\end{thm}
This paintbox representation is exactly the De Finetti mixing measure.
Dirichlet process is one example.

We can have more general subsets that overlap. Now a variable can belong to multiple subsets so can have multiple features.

\begin{thm}
A feature allocation is exchangeable iff it has a feature paintbox representation. 
\end{thm}
%This has to generate an exchangeable  
%has to be. De Finetti doesn't tell you how to find... in these particular cases it has a nice form. 
(You can also allow dust: you could never see the cluster again.)

We make the constraint that every slice has exactly 2 nodes.
How do you generate a usable form of this as a stochastic process? Use independence assumptions.
%rate measure factorizes.
\begin{thm}
A graph sequence is edge-exchange iff it has a graph paintbox.
\end{thm}
This extends to hypergraphs. You can add self-edges and skips.

If you don't want repeated edges this is not the model for you. You will get an infinite number of repeated edges!
You can also include dust: people communicate once and never again. %Nothing ever connects to everything else.

%what is nature of graph growth.
Is there another natural model that prohibits repeated edges? % expresses natural assumption that doesn't have repeated edges?

%Another type of exchangeability: measure of sets 
%labels of vertices

Picture: (Edge-exchangeable $\iff$ graph paintbox) $\supeq$ EVertexPF

How to prove sparsity?

We need the number of nodes to go to infinity. We need a countable infinity of latent nodes. 

The graph frequency model/vertex popularity model.
\begin{itemize}
\item
Draw a rate $w_i$ for each vertex $i$.
\item
Draw $\{i,j\}$ with probability $\propto w_iw_j$.
\end{itemize}
%draw 2d
Suppose $w_i \sim$PoissonPointProcess$(\nu)$ where $\nu$ is regularly varying. 
$$
\int_x^1 \nu(dw) \sim x^{-\al} l(x^{-1}),x\to 0, \quad \forall c>0\lim_{x\to \iy}\fc{l(cx)}{l(x)}=1.
$$
Then $|V_n|=\Te(n^\al l(n))$ and $|E_n|=\Te(n)$ a.s. 

It remains to fill out the Venn diagram.

Graph frequency models are a subset of edge-exchangeable models. (Ex. the 4-vertex model is in the difference.)
In fact, EVertex PF are the same as graph frequency models!

What's next: characterize all sparse, edge-exchangeable graphs. Characterize different types of power laws (edges, triangles, degree distributions, etc.). 

%lots of representations of these processes.
%Dir process useful, if you want to use variation Bayes, Hamiltonian Monte Carlo... There are a lot of different types of data out there only beginning to be explored.
%trees?
%ground truth?
%if exchangeable cluster, some probability associated with every cluster. Generalize in feature allocation case.
%general probability, way interact with other nodes.
%tell us about models at once.

%graphon always generate dense graphs.

Can this incorporate community structure? Vertex popularity. You can also do within graph paintbox.

Only certain subsets are exchangable (within communities)? You can get away from sparse assumption.

Do they model real world graphs? Can we simulate and get desirable behaviors? Can we put models together? %connectivity of Internet.
There are many other properties beyond sparsity you can be interested in.

%posterior on parameters
%email communities change over time.
%part of broader model changing over time. 
%observe new things because haven't seen yet, or have nonstationary process.
%multiple edges
%condition on not creating multiple edges.

%learn extra dimension.


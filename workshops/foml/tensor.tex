\section{	Tensor Decompositions for Learning Latent Variable Models, Daniel Hsu}

We give learning algorithms (parameter estimation) for latent variable models based on decompositions of moment tensors.

Examples:
\begin{enumerate}
\item
Summarize a corpus of documents. Documents express one or more thematic topics.

We want to know what topics are expressed, and how prevalent each topic is in the corpus. 

We can fit a latent variable model, a topic model (like latent Dirichlet allocation). 

Suppose there are $K$ topics, giving distributions over vocab words. A document is a mixture of topics. The word tokens in document are drawn iid from this mixture distribution.

The learning problem: given corpus of documents and hyperparameters ($K$), produce estimates of model parameters: For $t\in [K]$,
\begin{itemize}
\item
distribution $P_t$ over vocab words
\item
weight $w_t$ of topic $t$ in document corpus.
\end{itemize}
This would be just counting if each word token $x$ in document is annotated with the source topic $t_x\in [K]$. Then estimating the $\{(P_t,w_t)\}_{t=1}^K$ can be done directly.

Unfortunately, we don't have such annotation: topics are hidden. The direct approach to estimation is unavailable.
\item
%similarity between words?
%slew of unreasonable assumptions, still 
Subpopulations in data: Pearson (1894) measured the ratio of forehead-width to body-length for 1000 crabs. It was not a normal distribution.

Pearson hypothesized that it was 2 populations that are each gaussian. He used the method of moments to fit a mixture of 2 gaussians.

(Later it turned out to be just one species...)
\begin{align}
H&\sim \text{Discrete}(\pi_1,\ldots, \pi_K)\\
X|H=t&\sim \text{Normal}(\mu_t,\Si_t),\quad t\in [K].
\end{align}
We want to learn the mean and covariance for each, without knowing the classes of the data points.
\end{enumerate}
There is no direct estimators for MLE when some variables are hidden. The MLE is
$$
\te_{MLE}:= \amax_{\te\in \Te} \ln \Pj_\te\pat{data}.
$$
%minimize sum over data.
The log-likelihood is not necessarily a concave function of $\te$. This hasn't stopped anyone in ML or statistics: people use Expectation-Maximization (EM).

For Gaussian mixture models, the MLE is: given $\{x_i\}_{i=1}^n$, find $\{(\mu_t,\Si_t,\pi_t)\}_{t=1}^K$ to maximize
$$
\sumo in \ln \pa{\sumo tK \pi_t \rc{\det(\Si_t)^{\rc 2}} \exp\pa{-\rc 2 (x_i-\mu_i)^T \Si_t^{-1} (x_i-\mu_t)}}.
$$
This is sensible with restrcitions on $\Si_t\succeq \si^2 I$, but 
this is similar to the Euclidean $K$-means problem which is NP-hard.

Perhaps the instance you get is not worst-case. Look at the perspective of average-case analysis.

Suppose we get an iid sample of size $n$ generated from distributions with unknown parameters $\te\in \Te\subeq \R^p$, where $p$ is the number of parameters.

The task is to produce an estimate $\wh \te$ of $\te$ such that $\EE\ve{\wh \te - \te}\to 0$ as $n\to \iy$ (i.e. $\wh \te$ is consistent). (More precisely, $\Pj(\ve{\wh \te  - \te}\le \ep)\ge 1-\de$ in polytime.)

For small models ($K=2, \pi_t=\rc2, \Si_t=I$), EM can be consistent, but for larger $K$, it is easily trapped in local maxima far from global maxima. Practitioners often use EM with many random restarts.

It is hard to learn model parameters even when data is generated by model distribution. There is cryptographic hardness or information-theoretic hardness; you many need $2^{\Om(K)}$ running time or $2^{\Om(K)}$ sampling size. 

(Why do you want to estimate parameters; why not just get a close distribution? We want to learn something specific about the populations.)
%wrong metric on parameter distance.
%wrong metric on space of parameters. lack of robustness in parameters?

Ways around these barriers:
\begin{enumerate}
\item
Rule out hard cases with separation conditions. For example, assume
$$
\min_{i\ne j} \fc{\ve{\mu_i-\mu_j}^2}{\si_i^2+\si_j^2}
$$
is large
\item
Structural assumptions: sparsity, anchor words
\item
We assume non-degeneracy conditions: $\mu_i$ are in general position.
\end{enumerate}•
There is a broad class of techniques called method of moments that work.

The high-level idea:
\begin{enumerate}
\item
Determine function of model parameters $\te$ estimable from observable data, $\EE_\te [f(X)]$. Often third-order moments suffice.
\item
Form estimates of moments using data (iid sample)
$$
\wh \E[f(X)].
$$
\item
Approximately solve equations for parameters $\te$ $\EE_\te [f(X)] = \wh \E[f(X)]$. (Algorithms for tensor decomposition).
\item
Fine-tune estimated parameters with local optimization.
\end{enumerate}
%what do in misspecified case.

Model misspecification is an unresolved issue. %not same as finding low KL divergence.
We don't have a general methodology; now it is ad hoc, guided by examples. It's not clear how to incorporate general prior knowledge, and user feedback.

There are many other models amenable to moment tensor decomposition.

\subsection{Topic model for single-topic documents}

For simplicity, say there is a single topic for each document. The generative process is $t\sim \text{Discrete}(w_1,\ldots, w_K)$; given $t$ pick $L$ words from $P_t$. Given iid sample of documents of length $L$, produce estimates of model parameters $\{(P_t,w_t)\}_{t=1}^K$. 

How long must the documents be for identifiability?

%extreme Twitter
%When do we have identifiability? 
\begin{itemize}
\item
$L=1$: A random document is $\sim \sumo tK w_t P_t$. The parameters are not identifiable.
\item
$L=2$: regard $P_t$ as probability vector. The joint distribution of word pairs is given by matrix: A random document is $\sumo tK w_t P_tP_t^T$. The parameters are still not identifiable because the matrix decomposition is not unique.

You can also conclude this based on heuristic from parameter counting.
\item
$L=3$: Parameters are identifiable. A random document has three-wise co-ocurrences $\sim \sumo tK w_t P_t^{\ot 3}$,

\begin{clm}
If $\{P_t\}_{t=1}^K$ are linearly independent and all $w_t>0$, then parameters $\{(P_t,w_t)\}_{t=1}^K$ are identifiable from word triples. 
\end{clm}
This is implied by uniqueness of tensor decompositions.
\end{itemize}•

First, background on tensors. 

Matrices are tensors of order 2 which can be thought of as bilinear functions $M:\R^d\times \R^d\to \R$, $M(x,y) = x^TMy = \sum_{i,j} M_{ij}x_iy_j$.

A tensor of order $p$ is a $p$-linear function $T:\R^d \times \cdots \R^d \to \R$. Describe $T$ by $d^p$ values $T(e_{i_1},\ldots, e_{i_p})$. Identify $T$ with a multi-index array $T\in \R^{d\times \cdots \times d}$. 
%$$
%T(x^{(1)},\ldots, x^{(p)})=
%$$

Most tensor problems are NP-hard (Hillar, Lim 2013). It is remarkable that we can do anything at all.
%It's NP-hard even for rank 3.

Task: Given tensor $T=\sumo tK v_t^{\ot 3}$ with linearly independent components $\{v_t\}_{t=1}^K$, find the components up to scaling.
%promise problem

Jennrich's algorithm is based on collapsing the tensor. Think of $T:\R^d\times \R^d \times \R^d \to \R$ as $T:\R^d \to \R^{d\times d}$. (Cf. currying in functional programming.)
$$
[T(x)]_{j,k} = T(x,e_j,e_k).
$$
Collapse 2 different ways.
\begin{alg}[Jennrich]
\begin{enumerate}
\item
Pick $x,y$ independently and uniformly from $\bS^{d-1}$.
\item
Compute and return eigenvectors of $T(x)T(y)^{+}$. with nonzero eigenvalues.
\end{enumerate}
\end{alg}
For $T=\sumo tK v_t^{\ot 3}$, 
$$
T(x) = \sumo tK \an{v_t,x}v_tv_t^T = VD_xV^T.
$$
where $V=[v_1|\cdots |v_K]$ and $D_x = \diag(\an{v_1,x},\ldots, \an{v_K, x})$. By linear independence of $\{v_t\}_{t=1}^K$ and random choice of $x,y$, 
\begin{enumerate}
\item
$V$ has rank $K$
\item
$D_x,D_y$ are invertible a.s.
\item
diagonal entries of $D_xD_y^{-1}$ are distinct a.s.
\item
$T(x)T(y)^{+} = V(D_xD_y^{-1}) V^+$ a.s.
\end{enumerate}
So $\{v_t\}_{t=1}^K$ are the eigenvectors of $T(x)T(y)^+$ with distinct non-zeros eigenvalues. We actually want the $v_t$ to be linear independent enough, well-conditioned.

(Think of Jennrich as randomly combining different slices of tensor cube.)

We can generalize this to the asymmetric case. %You don't need all vectors to be linearly independent...

For the topic model, the probability of word triples is a third-order tensor
$$
T = \sumo tK P_t^{\ot 3} = \sumo tK v_t^{\ot 3}
$$
where $v_t=w_t^{\rc 3} P_t$. $\{v_t\}_{t=1}^K$ linearly independent means $\{P_t\}_{t=1}^K$ are linearly independent and $w_t>0$. 
We can recover $\{P_t\}_{t=1}^K$ and then $w_t$.

In conclusion, parameters of topic model for single-topic documents satisfying linear independence condition can be efficiently recovered from distribution of three-word documents. Two-word documents are not sufficient.
%estimate true probs. can estimate from true prob

It actually does something reasonable on real data: 300,000 NYT articles, 102660 words, $K=50$ topics. This is a 4-8 times speed-up over Gibbs sampling for LDA, comparable to FastLDA.

%not good ways to get confidence intervals? Use bootstrap in practice

%Estimate distribution of three-word documents gives $\wh T$, empirical moment tensor. Approximately decompose $\wh T$ to get estimates $\{(\wh P_t,\wh w_t)\}_{t=1}^K$. 
Some issues are accuracy of moment estimates (can more reliably estimate lower-order moments, get distribution-specific sample complexity bounds), robustness of approximate tensor decomposition (can use more error-tolerant decomposition algorithm, also computationally efficient), generality beyond simple topic models. 

\subsection{Moment decomposition for other models}

%Mixture of gaussians and linear regressions, multi-view algorithms

We will discuss two classical mixture models, and models with multi-view structure.

Consider mixtures of spherical gaussians
\begin{align}
H&\sim \text{Discrete}(\pi_1,\ldots, \pi_K)\\
X|H=t&\sim \text{Normal}(\mu_t,\si_t I_d),\quad t\in [K].
\end{align}
For simplicity, restrict to $\si_1=\cdots =\si_K=\si$.

The generative process is $X=Y+\si Z$ where $\Pj(Y=\mu_t) = \pi_t$  and $Z\sim N(0,I_d)$ is Gaussian noise.
The moments are
\begin{align}
\E X & = \sumo tK \pi_t \mu_t\\
\E(X\ot X) &= \sumo tK \pi_t \mu_t\ot \mu_t + \si^2 I_d.
\end{align}•
%shift eigenvalues of constant.
The span of the top $K$ eigenvectors of $E X\ot X$ contains $\{\mu_t\}_{t=1}^K$. 

Quantify separation by the number of standard deviations between component means
$$
sep :- \min_{i\ne j}\fc{\ve{\mu_i-\mu_j}}{\si}.
$$
Distance-based clustering (EM) works when $sep \succsim d^{\rc 4}$. Problem becomes $K$-dimensional via PCA when $sep\succsim K^{\rc 4}$, $K\le d$.

Third-order moments identify the mixture distribution when $\{\mu_t\}_{t=1}^K$ are linearly independent. Separation may be arbitrarily close to 0. People have also looked at general Gaussian and no minimum separation, but require $\Om(K)$th order moments (exponential complexity).

The third-order moment tensor is
$$
\E(X^{\ot 3}) = \E(\{Y+\si Z\}^{\ot 3}) = \E(Y^{\ot 3}) + \si^2 \E(Y\ot Z\ot Z + Z\ot Y \ot  Z + Z\ot Z\ot Y) = \sumo tK \pi_t \mu_t^{\ot 3} + \tau(\si^2, \mu).
$$
This by itself does not look useful.  But $\mu, \si^2$ are functions of $\E X$ and $\E(X\ot X)$, so we can estimate $\tau$. 
\begin{clm}
If $\{\mu_t\}_{t=1}^K$ are linearly independent and all $\pi_t>0$, then $\{(\mu_t, \pi_t)\}_{t=1}^K$ are identifiable from
$$
T:=\E(X^{\ot 3}) - \tau(\si^2,\mu) = \sumo tK \pi_t\mu_t^{\ot 3}.
$$
\end{clm}
Then you can use e.g. Jennrich's algorithm to recover $\{(\mu_t,\pi_t)\}_{t=1}^K$ from $T$.

If not linearly independent, you may need lots of data.

Note linear independence condition on $\{\mu_t\}_{t=1}^K$ requires $K\le d$.

Many works relax this to learn more mixtures of gaussians. They analyze mixtures of $d^{O(1)}$ gaussians with simple or known covariance via smoothed analysis and $O(1)$ order moments.
Ge, Huang, Kakade 2015 generalize to arbitrary unknown covariances. This is the overcomplete case.

Mixtures of linear regressions is another common mixture model
\begin{align}
H\sim \text{Discrete} (\pi_1,\ldots, \pi_K)
X&\sim N(\mu, \Si)\\
Y|H=t,X=x &\sim N(\an{\be_t,x},\si^2)
\end{align}•
The second-order moments give you a lot of information (assume $X\sim N(0,I_d)$)
\begin{align}
\E(Y^2 XX^T) & = 2\sumo tK \pi_t \be_t\be_t^T + \pa{
\si^2 + \sumo tK \pi_t \ve{\be_t}^2}I_d
\end{align}
The span of top $K$ eigenvectors contains $\{\be_t\}_{t=1}^K$.
Using Stein's identity, similar approach works for GLMs.


Tensor decomposition can recover parameters $\{(\be_t, \pi_t)\}_{t=1}^K$.

%param from O(1)

\subsubsection{Multi-view models}

In topic model for single-topic documents, there are $K$ topics $\{P_t\}_{t=1}^K$. Pick topic $H=t$ with probability $w_t$ (hidden). Words are $X_1,\ldots, X_L\sim P_H$ iid. The key property $X_1,\ldots, X_L$ are conditionally indepenent given $H$. Each word token $X_i$ provides a new ``view'' of hidden variable $H$.

Blum, Mitchell 1998 use this to justify co-training in semi-supervised learning. 

The views can be completely different; what's important is conditional independence.
$$
\E(X_1\ot X_2\ot X_3) = \sumo tK \pi_t \mu_t^{(1)}\ot \mu_t^{(2)} \ot \mu_t^{(3)}. 
$$
where $\mu_t^{(i)} = \E[X_i |H=t]$, $\pi_t=\Pj(H=t)$. Jennrich's algorithm works in the asymmetric case.

Examples of multi-view mixture models are
\begin{enumerate}
\item
Mixtures of high-dimensional product distributions
\item
Hidden Markov models
\item
Phylogenetic trees ($X_1,X_2,X_3$ are genes of 3 extant species, and $H$ is the LCA (lowest common ancestor) of most closely related pair of species)
\end{enumerate}•

%takeaway: recover from O(1) moments, exploit distributional properties, multiview structure to determine usable moments

We never exactly have the moments. We can only estimate moments with empirical moment tensor $\wh T$, and approximately decompose $\wh T$ to get parameter estimate $\wh \te$. We want algorithms that work for tensors that are close to having this form.

\subsection{Error-tolerant algorithms for tensor decompositions}

We can estimate $\E [X^{\ot 3}]$ from iid sample $\{x_i\}_{i=1}^n$, $\wh [X^{\ot 3}]:=\rc n\sumo in x_i^{\ot 3}$. We expect error of order $n^{-\rc 2}$ in some norm, $\ve{T}=\sup_{x,y,z\in \bS^{n-1}}T(x,y,z)$ operator norm or $\ve{T}_{F}:=\sum T(e_i,e_j,e_k)^2$ Frobenius norm.

We only have $\wh T$, with say $\ve{\wh T-T}\precsim n^{-\rc 2}$, $T=\sumo tK v_t^{\ot 3}$. 

Stability of eigenvectors requires eigenvalue gaps
$$
\De :=\min_{i\ne j} \ab{\fc{\an{v_i,x}}{\an{v_i,y}} - \fc{\an{v_j,x}}{\an{v_j,y}}}.
$$
We need $\ve{\wh T(x)\wh T(y)^+ - T(x) T(y)^+} \ll \De$ so $\wh T(x) \wh T(y)^+$ has sufficient eigenvalue gaps. We need $\ve{\wh T-T}_F\ll \rc{\poly(d)}$ for this.

We instead reduce to orthogonal case by considering a different set of moments.
In many applications we estimate moments of the form 
\begin{align}
M&=\sumo tK v_t\ot v_t\\
T&=\sumo tK \la_t v_t\ot v_t\ot v_t.
\end{align}
Assume $v_t$ linearly independent, $\la_t$ positive.
$M$ is positive semidefinite of rank $K$ so determines inner product system on $\spn\{v_t\}_{t=1}^K$ such that $\{v_t\}_{t=1}^K$ are orthonormal. The process is \vocab{whitening}.

Our goal: given $\wh T\in \R^{d\times d\times d}$ such that $\ve{\wh T- T}\le \ep$ for some $T=\sumo td \la_t v_t^{\ot 3}$ where $\seqo td{v_t}$ are orthonormal and $\la_t>0$, approximately recover $\seqo td{(v_t,\la_t)}$. 

The analogous matrix problem:
\begin{enumerate}
\item
For $\ep=0$ this is eigendecompsotion. %exists by symmetricy
unique if $\seqo td{\la_t}$ distinct. 
\item
For $\ep>0$ use perturbation theory for eigenvalues (Weyl) and eigenvectors (Davis and Kahan).
\end{enumerate}•

First assume $\ep=0$ so $\wh T=T$. Try to match moments by optimization
$$
\seqo td{(\wh v_t,\wh \la_t)} :=\amin_{\seqo td{(x_t,\si_t)}}\ve{T-\sumo td \si_t x_t^{\ot 3}}_F^2.
$$
Greedy approach is to find best rank 1 approximation, deflate $T:=T-\wh \la \cdot \wh v^{\ot 3}$ and repeat. For orthogonal decomposition, this works!
(Note the optimization is equivalent to $\wh v = \amin_{x\in \bS^{d-1}} T(x,x,x)$.)

\begin{clm}
The local maximizers of the function
$$
x\mapsto T(x,x,x) = \sumo td \la_t \an{v_t,x}^3
$$
 are $\seqo td{v_t}$ and $T(v_t,v_t,v_t)=\la_t$.
\end{clm}
The algorithm is to use gradient ascent to find each component $v_t$.

A slightly different algorithm, nicer, is parameter-free fixed-point algorithm, due to De Lathauwer, De Moore, Vandewalle 2000. The first-order (necessary but not sufficeint) optimality condition is $\nb_x T(x,x,x)=\la x$. The partial evaluation of $T$ is 
$$\nb_x T(x,x,x) = 3\sum_{i,j} T_{i,j,k}x_ix_je_k = 3T(x,x,\cdot).$$
The third-order tensor power iteration is
$$
x^{(i+1)} = \fc{T(x^{(i)}, x^{(i)}, \cdot)}{\ve{T(x^{(i)}, x^{(i)}, \cdot)}}.
$$
This is only in the orthogonal case.
This looks like matrix power iteration but has different properties.
%nv

For matrix power iteration $x^{(i+1)}=\nv{Mx^{(i)}}$, we require gap $\min_{i\ne 1} 1- \fc{\la_i}{\la_1}>0$ to converge to $v_1$. For tensor power iteration no gap is required (note the solution is unique even without a gap).

Where we converge depends on the initialization.  In matrix power iteration, if $\an{v_1,x^{(0)}}\ne 0$, gap $>0$, then converges to $v_1$.
If $t:=\amax_{t'} \la_{t'} |\an{v_{t'},x^{(0)}}|$, it converges to $v_t$. 
Matrix power iteration converges at linear rate and tensor power iteration converges at quadratic rate.

Now allow $\ep>0$.  
\begin{clm}
For $\wh v:= \amax_{x\in \bS^{d-1}} \wh T(x,x,x)$,
$$
|\wh \la - \la_t|\le \ep, \ve{\wh v-v_t} \le O\pa{\fc{\ep}{\la_t} + \pf{\ep}{\la_t}^2}
$$
for some $t\in [d]$, $\la_t\ge \max_{t'}\la_{t'} - 2\ep$.
\end{clm}
There are many efficient algorithms for solving this approximately when $\ep$ is small enough ($\rc d, \rc{\sqrt d}$). Think of $d$ as $k$ in the original problem.

We accumulate errors from deflation. (If the $\la$'s are the same, we can just restart with the original tensor, instead of deflating and recursing.)
$$
\wh T - \wh v_1^{\ot 3} = \sum_{t=2}^d v_t^{\ot 3} + E + (v_t^{\ot 3} - \wh v_1^{\ot 3}).
$$
Error seems to have doubled. %But it's a lower-order effect.
\begin{align}
\ve{\rc 3\nb_x \ba{(v_1^{\ot 3} - \wh v_1^{\ot 3})(x,x,x)}} &= \ve{\ve{v_1,x}^2 v_1-\an{\wh v_1, x}^2\wh v_1}\\
&=\an{\wh v_1,x}^2\\
&\le \ve{v_1-\wh v_1}^2 \le \ep^2.
\end{align}
So effect of errors in directions orthogonal to $v_1$ is $(1+o(1))\ep$ rather than $2\ep$. Deflation errors have lower-order effect on finding other $v_t$: analogous statement for deflation with matrices does not hold.

We can also use alternating minimization, but don't know how to analyze it theoretically.

Recap: reduction to nearly orthogonal decomposable tensor permits simple and error-tolerant algorithms. Differences from matrix decomposition (nonlinearity) are crucial.

%Using method-of-moments with $O(1)$-order moments, we can efficiently estimate parameters for many latent variable models. 

There are many issues to resolve: handle model misspecification, increase robustness; develop general methodology, incorporate general prior knowledge, incorporate user feedback interactively.

Unlike in some other settings, here the data have to be plausibly generated from the assumed model.

Main use of these methods may be to initialize local search procedure.

See Anandkumar, Ge, Hsu, Telgarsky, Tensor decompositions for learning latent variable models, 2014, and Moitra, Algorithmic aspects of machine learning, 2014 (Chapter 3).

%ident up to rotations, etc.
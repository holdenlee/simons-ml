\section{Power of Active Sampling for Unsupervised Learning (Aarti Singh, Carnegie Mellon University)}

Abstract: Most modern datasets are plagued with missing data or limited sample sizes. However, in many applications, we have control over the data sampling process such as which drug-gene interactions to record, which network routes to probe, which movies to rate, etc. Thus, we can ask the question ? what does the freedom to actively sample data in a feedback-driven manner buy us? Active learning tries to answer this question for supervised learning. In this talk, I will present work by my group on active sampling methods for several unsupervised learning problems such as matrix and tensor completion/approximation, column subset selection, learning structure of graphical models, reconstructing graph-structured signals, and clustering, as time permits. I will quantify the precise reduction in the amount of data needed to achieve a desired statistical error, as well as demonstrate that active sampling often also enables us to handle a larger class of models such as matrices with coherent row or column space, graphs with heterogeneous degree distributions, and clusters at finer resolutions, when compared to passive (non-feedback driven) sampling.

Despite the availability of big datasets, we can seldom hope to measure our systems everywhere and all the time. For example there is too much data in computer networks (too many node), plenoptic camera (record spectral characteristics, etc. of materials), and smart cities. When we have a limit on how much data we can collect, what do we do?

Differentiate
\begin{enumerate}
\item
passive sampling: data is drawn uniformly at random, or selected prior to observing any data (experimental design). Ex. given graph of connections, choose where on the graph to sample.
\item
active sampling: selective data drawn sequentially in a feedback-driven way. (For many applications we have control over what data to acquire.) Ex. select which nodes to ping, which biomedical experiments to do.
\end{enumerate}•
Tradeoffs in statistical learning are
\begin{enumerate}
\item
statistical efficiency (error, risk, noise tolerance)
\item
measurement efficiency
\item
memory efficiency
\item
computational efficiency
\item
communication efficiency
\item
model assumptions
\end{enumerate}•
How can active sampling enhance these tradeoffs? %be clever about which measurements obtain?

Most of active learning has been in the context of supervised problems (classification, regression), whether to collect the label for a data point. 

If there is (unknown) heterogeneity, active methods can learn about it using feedback and adapt the sampling.  (If heterogeneity is known, you can just design the sampling at the beginning.)
In heterogeneity, points near decision boundary are more important.
Active sampling helps for piecewise smooth functions

What heterogeneities might exist and how to exploit them?

We talk about 3 problems and their heterogeneities.
\begin{enumerate}
\item
Graphical model structure learning: varying node degrees.
\item
Matrix, tensor completion and approximation, column subset selection: spiky columns or rows
\item
Hierarchical clustering: multi-resolution clusters
\end{enumerate}•

\subsection{Graphical model structure learning}

We want to learn conditional independence relations between variables (ex. sensors, protein interaction network) using few samples/measurements.

Graphical models encode
$$
\{i,j\}\nin E\iff X_i\perp X_j|X_{[p]\bs\{i,j\}}.
$$
Two nodes are not connected by an edge iff they sare independent given all others.
Gaussian graphical models have $X_1,\ldots, X_p\sim N(0,\Si)$ iid.

$K=\Si^{-1}$ is sparse with zeros encoding absence of edges (conditional independencies).

Lower bound (Wang-Wainwright-Ramachandran10): Number of passive samples necessary is $np=\wt\Om_{d_{max}}(d_{\max}p\ln p)$. 

Upper bound (Meinhausen-Buhlmann06, Wainwright09): $np=I(d_{\max} p\ln p)$. Use node-wise lasso.

Passive sampling requires sampling from the joint distribution. Collecting these samples is expensive: there is a cost of measurement and synchronization.
In practice, you may not be able to do this.

Active sampling: Select a subset and sample from the marginal,
$$
\{X_{S_l}^{(i)}\}_{i=1}^{N_l}\sim N(0,\Si_{S_l})
$$
Get $\sum_l N_l|S_l|$ samples. 

%Take some samples 
First take a small number of samples from all variables, get an estimate of their neighborhoods. For the ones whose neighborhoods you learn, use half your budget to learn it and half to refine, then stop sampling from them.

Initialize NBDFound, Settled, $D_1,D_2=\phi$, $l=1$.
Obtain $N=cl\ln p$ samples from $X_{[p]\bs\pat{Settled}}$ and add to $D_1$< repeat for $D_2$. Sequentially for each $j\in [p]\bs\pat{NBDFound}$, 
\begin{enumerate}
\item
learn neighborhood: identify top $\le l$ size neighborhood of $j$ using $D_1$ via lasso
\item
verify neighborhood: if given learnt neighborhood, the partial correlation (computed with $D_2$) of $j$ with remaining nodes $\le \xi$, add $j$ to NBDFound.
\end{enumerate}
For $j\in \pat{NBDFound}$ if all neighbors of $j$ are in NBDFound, add $j$ to Settled.

You get more samples from the hub nodes. 

Let $d_{\max}(j)$ be the max degree of any neighbor of node $j$. 
The number of active samples sufficient is %can be smaller:
$$
\sum_l N_l|S_l| = O\pa{\sumo jp d_{\max}(j)\ln p}
$$
The number of samples of a node needed is adapted to its local degree. This is good for skewed degree distributions and minimizes need for synchronized measurements of variables.
%preferential attachment 

In experiments we see improvement for power law (preferential attachment) graphs.

We have a lower bound with average degree (rather than average max degree of neighbor).

\subsection{Active sampling for matrix completion}

Complete or approximate a matrix by a low-rank matrix given feedback-driven choice of which entries to observe.

Usual assumption is that the matrix is both row and column incoherent. Active sampling helps when it is spread out in 1 direction, ex. column incoherent. 
For a $k$-dimensional subspace $U\subeq \R^n$, 
$$
\mu(U) = \fc nk \max_{i\in [n]} \ve{P_Ue_i}_2^2.
$$
Here $\ve{P_Ue_i}_2$ is unnormalized leverage score of $i$th coordinate if $U$ is top $k$ principal components.
Incoherence means that energy in the space is uniformly spread out amongst coordinates.
%Relevant are incoherence and leverage scores.

Lower bounds due to Candes-Tao10: The number of random entries necessary to recover $n\times n$ matrix of rank $r$ with row and column space coherence bounded by $\mu$ scales as $\mu r\ln n$ per column.

Upper bound (Yudong-Chen15): the number of random entries sufficient is $\mu r(\ln n)^2$. Use nuclear norm regularization, $\min_X \ve{X}_*$, convex surrogate.

What does active sampling do. Initialize subspace $A=\{0\}$.
\begin{enumerate}
\item
Randomly draw $m$ entries from column.
\item
If column doesn't lie in space spanned by $A$, observe all entries, add to $A$. (Test using partially observed column, but fully observed subspace.)
\item
Else complete it by projecting onto current $A$.
\end{enumerate}
If $\mu \asymp \mu r (\ln \pf r\de)^2$ where column space has incoherence $\le \mu$, then algorithm recovers wp $\ge 1-\de$ using $n_2m+rn_1$ entries ($\sim n\mu r(\ln r)^2$). %and runs in time $n_2mr

Measurement complexity goes from $\mu r(\ln n)^2$ to $\mu r(\ln r)^2$, independent of matrix size.
Computational complexity goes from $n^2r$ (doing things with singular values) to $(nr^2+r^4)(\ln r)^2$ (sequential)
If learns in 5 minutes rather than 2 hours!
It does sequential column/row processing. Memory complexity goes from $n^2$ (which can be improved) to $nr$. The most incredible is that you've increased the class of matrices you can handle. Random sampling requires incoherent rows and columns; active sampling only requires incoherent rows OR columns. It focuses on harder examples.

%If I don't know if my matrix is row or column incoherent.

Lower bound for passive sampling: number of passive entries necessary to recover $n\times n$ matrix of rank $r$ with only column space coherence bounded by $\mu$ scales as $n$ per column.

For row coherent matrices, random sampling requires $n$ per column; we only require $\mu r(\ln r)^2$ per column.

What if matrix is not exactly low-rank? %Approximate matrix. 
I'll show this in the context of column subset selection. Can I find columns that give the best rank $k$ approximation? (Find features that are most representative.)
%Estimate column norm, observe in full.

We get a a $1+3\ep$ approximation with respect to best residual norm. A lot of matrix approximation results get additive error. 
The sample complexity is similar to passive matrix completion, but the error bound is relative. 
We only assume column space incoherence.

\subsection{Hierarchical clustering}

Cluster objects given feedback-driven choice of whichpairwise similarities to observe.

For hierarchical clusters, need $n\ln n$ similarities; for heterogeneous clusters, just need $nk^2$ similarities. ``Break the $\sqrt n$ barrier for stochastic block models.''

%People have generative models for graphs. What incoherence would you expect?

%mix active and passive. in theory back to passive. in practice, do this all the time.

%Column subset selection: Fully observed, don't need incoherence. You need incoherence to get something with partially observed data.
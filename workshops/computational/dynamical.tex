\section{Gradient Descent Learns Linear Dynamical Systems (Moritz Hardt, Google)}

We prove that gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.
 
Joint work with Tengyu Ma and Ben Recht.

Does SGD learn a linear dynamical system? Yes!

We have a distribution over sequences $\{(x_t,y_t)\}_{t\in [T]}$. 
Population risk $$f(\wh \te )  =\E_{\{x_t\},\{\xi_t\}}\ba{\rc T \sumo tT \ve{\wh y_t-y_t}^2}$$
where $\wh \te = (\wh A, \wh B, \wh C, \wh D)$ is the parameters of learning system. 

$\wh y_t$ is the prediction of the  learned system from the correct initial state.

Population risk is non-convex. Quasi-convexity is sufficient condition for convergence of SGD,
$$
\an{\nb f(\te), \te - \te^*} \ge \tau(f(\te) - f(\te^*)),
$$
``gradient correlated with descent direction''. 

Under what conditions is risk quasi-convex?
We need terminology from control theory.
Controllable canonical form is $\matt{0}{I}{-a_n}{-[a_{n-1},\ldots ,a_1]}$, $B=[0,\ldots, 0,1]^T$, shift with linear combination at bottom. There are only $n$ relevant parameters, and we don't need to worry about $B$. The bottom row tells exactly what the characteristic polynomial is, $z^n+a_1z^{n-1}+\cdots + a_n$. 
Stability (inputs don't blow up) means $\rh(A)<1$. 

Polynomial roots condition (Pac-Man condition): consider $q(z) = 1+a_1z+\cdots + a_nz^n= z^n p_a(z^{-1})$. We require $\Re(q(z)) > |\Im (q(z))|$ for all $|z|=1$.

This ensures stability of system (Rouch\'e), convexity of constraint region, and quasi-convexity of population risk.

Assume Pac-Man, pairwise independent inputs with mean 0 and variance 1, iid noise with mean 0 and variance $\si^2$.  %with a little slack.
Projected stochastic gradient descent with $N$ samples of length $T$ learns unknown system of order $n$ with $f(\wh \te) \precsim \sfc{n^5+\si^2n^3}{TN}$.

Overparametrization helps. Learn order $n$ system with $n'>n$ parameters. Replace Pac-Man with weak separation of roots condition.
Note for large enough $n'$ problem turns into linear regression. 

Proof outline:
\begin{enumerate}
\item
Relate objective function to transfer function of system.
\item
Use properties of transfer function to show quasi-convexity.
\end{enumerate}
First step: Unroll objective function using definition.
$$
f(\wh \te) \approx \ve{D-\wh D}^2 + \sumz k\iy (\wh C\wh A^k B - CA^k B)^2.
$$
Define idealized risk
$$
g(\wh A, \wh C) = \sumz k\iy (\wh C\wh A^k B - CA^k B)^2.
$$
Transfer function is $G(z) = \sumo t\iy z^{-t} CA^{t-1}B$.
Evaluate at a point on complex unit circle. Get nice curves which correspond to useful values of $\te$.

Pass from time to frequency domain. If $\rh(\wh A)<1$,
$$
g(\wh A, \wh C) = \int_0^{2\pi} |\wh G(e^{i\te}) - G(e^{i\te})|^2\,d\te.
$$
Proof: $G(e^{i\te})$ is Fourier transform of $\{CA^{t-1}B\}$. Parseval's identity.

%Instead of look at power series, just look at distance in $L^2$ norm. 

How does this help establish quasi-convexity?

Lemma: $G(z) = C(zI-A)^{-1}B = \fc{c_1+c_2z+\cdots +c_nz^{n-1}}{z^n + a_1z^{n-1}+\cdots +a_n}$.

In which domain is $(G(z)-\wh G(z))^2$ quasi-convex?
This reduces to the case $f(\wh u, \wh v) = \pa{\fc{\wh u}{\wh v}-\fc uv}^2$. 
It's quasiconvex over $\set{(\wh u,\wh v)}{\fc{\wh v}{v}>0}$. 

Idealized risk is quasi-convex under PacMan condition. S\''oderstr\''om, Brief paper (1973). Some properties of the output error method (1982). 

\begin{itemize}
\item
SGD solves SysID under PacMan condition. What condition is sufficient and necessary?
\item
Overspecification helps a lot, eventually becomes linear regression. (When?)
\item
Right dependence on $N$, $T$. What about dependence on $n$?
\item
Build theory for sequence learning. What can we learn from control theory?
\end{itemize}
%initialize outside? nonconvex? 
%what about agnostic setting?


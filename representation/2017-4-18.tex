\section{Efficient Optimization of a Convolutional Network with Gaussian Inputs (Amir Globerson)}

Deep learning models are often successfully trained using gradient descent, despite the worst case hardness of the underlying non-convex optimization problem. The key question is then under what conditions can one prove that optimization will succeed. Here we provide, for the first time, a result of this kind for a one hidden layer ConvNet with no overlap and ReLU activation. For this architecture we show that learning is hard in the general case, but that when the input distribution is Gaussian, gradient descent converges to the global optimum in polynomial time. I will additionally discuss an alternative approach to sidestepping the complexity of deep learning optimization using improper learning.

Deep learning has a powerful class of models, but the training is nonconvex. This is sometimes a problem, but often not. Theory says that neural nets ``shouldn't work'', but now in practice they work.

What has changed? Much more data, different architectures.

There are many hardness results (Blum, Auer, Shamir, Livni). Choromanska et al. show no bad local minima under strong decoupling assumptions (independence between inputs and activations, it's unclear there are settings where this holds!).

Kawaguchi (NIPS 2017): neural nets with linear activations do not have local optima. Extensions to ReLU case removing some assumptions by Choromanska.

Non convex optimization: Matrix factorization globally optimal with gradient descent (Ma et al., NIPS 2017).

Ge et al (COLT 2015), Lee et al (COLT 2016): efficient optimization of objectives with strict saddle points using noisy gradient descent. Saddle points are not an issue.

%Apart from global min show saddle points

We do not expect arbitrary neural architecture to be globally optimized for arbitrary distributions. We want to find a case with architecture used in practice, optimization algorithm used in practice, and prove that global optimum can be found. We need more assumptions about the data distribution. 

Conv nets have parameter tying and sparsity (weights only applied locally). 

Are convnets easier/harder to learn? Under which conditions? Not much is known. We study a simple model.

Simple model of convolutional layer.
\begin{itemize}
\item
Learning with arbitrary inputs is hard.
\item
Learning with Gaussian inputs is tractable using gradient descent.
\end{itemize}

Non-overlap model. No overlap between different parts of input where (the same) filters are applied (so single channel). ReLU on top of filters, average pooling on top. There's hope of doing this with multiple channels.
We want to understand optimization under parameter tying.

Capture some key elements (but not all): parameter tying, nonlinear activation, pooling. Peter Bartlett: for fully connected model with Gaussian input, if weights are orthogonal, they can show some results. Under some assumptions, there's hope. 

Consider the realizable case. Input features $x$ are generated by some distribution $D$. Output $y$ produced using true weights $w^*$. Goal is to minimize squared loss $\ell(w) = \E_{x\sim D} [(f(x;w)-f(x;w^*))^2]$. 

%(phase retrieval?)

This is NP-hard to $\ep$-approximate for $\ep=\rc{4dk^5}$. Reduce from $k$-set splitting problem. Given $S$ with $d$ elements, subsets $C_1,\ldots, C_m$, find non-overlapping $S_1,\ldots, S_k$ that cover $S$ and do not completely cover $C_i$.
Convert to learning problem with inputs $x\in \R^{k^2d}$, $w\in \R^{kd}$. Think of $w=[w^1,\ldots, w^k]$, $w^i\in \R^d$ indicator for subset $S$.

For subset $v\in \R^d$ define $d(v)\in \R^{k^2d}$ by planting $k$ copy, one in each $\R^{kd}$.

Output measures to what degree does $v$ agree with any of the subset in $w_*$.

Training set is $x_i=d(e_i)$, $y_i=\rc k$. This says there exists some set in $w$ containing $i$. Also non-cover condition.
%No subset in $W$ completely co

Splitting sets imply Non-overlap.

Converse: $S_i = \set{j}{w_j^i \ge \rc k}$. Threshold. %$f(x_i;w)=\rc k$ 

We weren't sure of difficult of optimizing. Once you have latent, hidden layer, and you just observe pooling, you don't know which part it came from. %For a given output 

%come up with examples hard to optimize.
Random examples are often easy to optimize; it's hard to find examples that are hard to optimize.

%random-sat more interesting
%average-case hard for SAT, propagate through reduction.
%only hard in certification sense, measure 0 set.
%exactly at threshold?
%planted SAT

Because optimizing non-overlap models is worst case hard, we need assumptions on generating distribution $G$.

Useful integral from Cho and Saul for Gaussian distribution:
$$
\E_{G} [\si(u\cdot x)\si(v\cdot x)] = \rc{2\pi }\ve{u}\ve{v}\pa{\sin \te + (\pi - \te)\cos\te},
$$
$\te$ the angle between $u,v$.

Denote this by $g(u,v)$. Expected loss is 
$$
\ell(w) = \E_{\cal G}[(f(x;w) - f(x;w^*))^2]
= \rc{k^2} [\ga\ve{w}^2 - 2k g(w,w^*) - 2\be \ve{w}\ve{w^*}].
$$

Gaussian loss has nondifferentiable max at 0, global min at $w^*$, and degenerate saddle point at $-\al w^*$. 
Will gradient descent globally converge? Existing results can't be used because of degenerate saddle points.

\begin{thm}
For $0<\ep<\ep_0$, gradient descent will converge to loss $\le \ep$ in $O(\ep^{-2})$ iterations with high probability.
\end{thm}

Key ideas:
\begin{enumerate}
\item
Bad points are avoided.
\item
Function will have Lipschitz continuous gradient along path of algorithm.
\item
Use this to show gradient descent is optimal.
\end{enumerate}

Gradient update is
$$
w_{t+1} = (1+\la c_1(w_t,w^*)) w_t + \la c_2(w_t,w^*) w^*
$$
where $c_1>-1$, $c_2>0$. Assume $\la\le 1$.
Angle $\angle(w_t,w^*)$ decreases.

The property of Gaussian we use is the correlation (integral) formula.

Empirical illustration: generate $w^*$ from set splitting problem. Generated training set with this $w^*$ but Gaussian inputs. Gaussian can be learned; non-Gaussian cannot.

Networks with overlap: there are local minima. But basin of attraction is non-negligible. You can find it with restarts. Conjecture: number of restarts doesn't grow with dimension.

We're analyzing gradient descent,  not SGD.

%Concentrate quickly?

Open questions: 
\begin{itemize}
\item
finite sample analysis. Can be done with measure concentration results.
\item
Max pooling
\item
Analysis of overlapping case.
\item
Classification instead of regression. (Another nonlinearity on top.)
\end{itemize}â€¢
Learning intersection of halfspaces (Baum, Klivans). Under Gaussian and more generally log-concave distributions it can be learned (but not with gradient descent). 

%conv, no redundancy issues. Don't have symmetries

%\subsection{Improper deep learning}


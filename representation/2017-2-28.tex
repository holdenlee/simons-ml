\section{Topic models: Proof to Practice (Ravi Kannan)}

Check out the book ``Foundations of Data Science.''

Topic Models posit a stochastic generation process for document corpora and devise algorithms to learn the model from real data. Currently, there are two methods of validation: improved efficiency on benchmark corpora up to billions of words and mathematically proven error and time bounds tested on smaller cases. I will present our recent effort where the two meet. The main new algorithm ingredient is an importance sampling procedure inspired by Randomized Linear Algebra. Whereas known topic models posit a near low-rank data matrix, we start with a new high-rank model which allows for realistic noise. The algorithm empirically performs better to scale  than the state of the art.

\subsection{Problem}
\begin{itemize}
\item
The input is a corpus of $s$ documents ($>10^6$). Each is a $d$-vector of word frequencies ($d>5000$). 
\item
Assume there are $k$ unknown topics ($>500$) unknown topics, and each document is approximately a convex combination of topics.
\item
Find topics. This is NP-hard in general. %hidden hand
Assume there is a generative model (``hidden hand'').
\item
LDA model: For each document, use a Dirichlet distrution to generate a $k$-vector of topic weights and take that weighted combination of topics to get a probability distribution over words. Now draw words from that distribution to create the document. You only see the frequency of the words generated.
\item
You can put any prior on the weights; or you may not put a prior at all.
% a weighted combination of topics. 
%generate topics using
\end{itemize}
For this talk $k$ is given.

Think of the input as a matrix of words$\times$documents. The output is a words$\times$topics number.
Ex post facto, a human can sit down and assign names to the topcs, like ``politics''. The topics are not named beforehand.

This is unsupervised learning.

Think of the word-topic vectors $\mu_i$ as corners of a simplex. The ``hidden hand'' picks probability vectors for each document, in the simplex. Now generate a document according to the multinomial distribution of those probability vectors. The word-document vectors could be way outside the simplex---there is enormous variance: the number of words in the dictionary could be much larger than the document size.
%\sqrt D

%need a lot of documents before you see cooccurrences.

%anchor 2
In a different model, you can get by with just 3 words per document (Anandkumar).

%gen $m$ words for each doc in iid trials according to the weighted combintion of topics.

The learning problem:
\begin{itemize}
\item
Find maximum likelihood estimator of topic matrix.
\item
Find hidden variables: topic weights vector for each doc, or even the topic assignment of each word in each document.
\item
\end{itemize}
A standard algorithms is to  run a Markov chain on $k^{sm}$ states. 

There are many applications, and empirical successes in speeding this up, but no proof of fast convergence.
%how measure success?

%generative data. Generative under model you assume.

\subsection{Provable algorithms}

What have people focused on?
\begin{itemize}
\item
ML, statistics:
\begin{itemize}
\item
Model. There are often nice statistical/mathematical underpinnings.

Ex. The Dirichlet distribution is the conjugate prior to the  multinomial.
\item
Simple, efficient algorithms: Lloyd's $k$-means, Expectation-minimization, Gibbs sampling, alternating minimization, Metropolis-Hastings
\item
Simple proofs of convergence to local minima. But often there are no proofs of finite sample error, time bounds, or closeness to global optima.
\end{itemize}
\item
Theory
\begin{itemize}
\item
Prove that if the data was generated by a hidden model, then in poly time, approximate the generating model from data, ideally with widely used or usable algorithms. (Here we abbreviate ``with proven poly time and error bounds'' by ``provable''.)
\item
Gaussian Mixture Modeling: Dasgupta Schulman; Arora Kannan, Vempala, Wang; Kalai, Moitra, G Valiant
\item
Kumar, Kannan; Awashti, Or. Alternating minimzation, Jain, Netrapalli; Ge,...
\end{itemize}•
\end{itemize}
The agnostic case is harder.
%Lloyd also not generative model. 

Topic modeling is a case of non-negative matrix factorization. Exact NMF is that we are given $A=BC$ with all nonnegative entries and asked to find $B,C$. 
\begin{itemize}
\item
Arora, Ge, Kannan, Moitra: Provable algorithms for NMP (under assumptions)
\item
Arora, Ge, Moitra,...: Provable algorithm for topic modeling (under assumptions)
\item
Improvements: Recht, Re, Tropp, Bittoff; Gilis. Improvments on NMF algorithms
\item
Bansal, Bhattacharyya, Kannan: under realistic, empirically verified assumptions
%
\item
Bh, Goyal, Kannan, Pani: Provable algorithm for NMF under a realistic noise model (noise swamps data), empirical performance (ICML2016)

Previous algorithms assume noise for a data point is $\ep$ times the data point.
\item
Bh, Kannan, Simhadri, Dave, Horsala: Provably learning deep topics from a billion tokens.
\end{itemize}

Provable algorithms for exact NMF: Intuition: if there was one $x$ at each corner, we can find them by one linear program per data point. You can improve this to solving just one convex problem. This is not scalable. 

This is not robust to noise---if you only rely on one point, and the point is wrong...

\subsection{TSVD}

Our model makes the assumptions:
\begin{enumerate}
\item
Each topic has a set of catchwords. 
\begin{itemize}
\item
Each catchword has a higher frequency in the topic than in other topics.
\item
All catchwords for a topic together have frequency at least a constant, say 0.1.
\item Frequency of each catchwords in a topic (ex. batter, bases, homerun in baseball) is $\ge 1.1$ times frequency in any other topic. %and together their 
\item
This replaces the anchor words assumptions of earlier papers.

If ``homerun'' is an anchor word for baseball, it only occurs in the baseball topic and it occupies a constant frequency (ex. 0.1 of the words).
\end{itemize}
\item
Each document has a dominant topic whose weight is say $\ge 0.2$ (when $k=100$) and the weight of each other topic in document is $\le 0.15$.
\item
Nearly pure documents: for each topic, there is a fraction, say $\rc{10k}$ whose weight on the topic is at least 0.9.
\end{enumerate}

All three assumptions are empirically verified on real data. 
Provably, under the traditional LDA model with reasonable hyperparameters,  a majority of docs have dominant topics and a fraction are nearly pure.

SVD gets a bad rep:
\begin{itemize}
\item
Latent semantic indexing: Dumais... 1990 propose SVD as denoiser.
\item
Papadimitriou, Raghavan, Tomaki, Vempala: LSI provably does Topic Modeling when each doc is purely on one topic.
\item
Unintended consequence: SVD does not help when there are multiple topics per document (folklore)
\item
Arora, Ge, Moitra, Beyond SVD: LP based algorithm for topic modeling.
%who's afraid of svd
%2 svd suffices
%\item
%Tensor-based algorithm.
\end{itemize}•
SVD and spectral decomposition do scale up; there are good algorithms from numerical analysis. But truncated SVD to rank 10000+ with $10^9+$ nonzeros still takes significant computing power.
%pagerank, etc.

SVD mixes with negative weights, so solely relying on it is a mismatch to the model.

Here is our unsampled threshold SVD (TSVD) algorithm.
\begin{enumerate}
\item
Threshold data. (For each word find a threshold and set the frequencies of the word to 0 if below the threshold.)
\item
SVD and cluster by dominant topic. (Use $k$-means clustering on the thresholded topics. There is no convergence in general. We leverage earlier theory: we prove that clustering in SVD projection gives a good start.) %clustering divides up the simplex.
(We are not done: we cannot just take the average in each cluster because we need the corners.)
\item
Find catchwords (most frequent words in each cluster). (We prove that words with $1-\rc k$-fracile frequency in this cluster that is $>1.1$ times other clusters are approximately the set of catchwords for the topic.)
\item
Find pure documents (docs with highest total frequency of catchwords for each topic among all docs). 
\end{enumerate}•
Why TSVD?
\begin{enumerate}
\item
Provable bounds on reconstruction error and time
\item
Performs better on many quality metrics
\item
Fast in practice and parallelizable.
\end{enumerate}•

\subsection{Deep topic models}
%Why not for word embedding? (Prove word embeddings are the right answer.)
%
Now I want to scale up to larger $k$. 

The limitation of known models: Suppose we are given $d\times s$ data matrix $A$, find $d\times k$ topic matrix $M$ for which there is high (nearly max) likelihood. Assume no prior on the topic weight matrix $W$. We have the problems
\begin{enumerate}
\item
If $k=d$, $M=I$, $W=A$ (each topic is a single word) is the MLE.

Zipf's law says that there exists $d_0=o(d)$, $d_0$ high frequency words form $1-\ep$ of data.

MLE is still trivial if $k\ge d_0$.
\item
With large $k$ number of free parameters grows, overfitting
\item
Sample complexity grows as high power of $k$.
\end{enumerate}
We replace the model.
\begin{itemize}
\item
Assume there are $k_0\ll k$ basic topics and each deep topic  (ex. primary and secondary classification) consists of one or two (or at most $q$) basic topics. 

This is like a hierarchical model. 

The number $k$ of deep topics satisfies $k_0\le k=O(k_0^q)$. 
\item
Set of catchwords for basic topics is defined as before. There is a set of keywords for a deep topic: weighted combination of sets of catchwords comprising its basic topics.
\item
This solves the problem of $k$ being large: 
We do topic modeling on $k_0$, not $k$ topics.
\item
Matrix formulation: $d\times k$ topic matrix $M$, $d\times k_0$ $M^{(1)}$ of basic topics, 
$$
M = M^{(1)}M^{(2)}
$$
where $M^{(2)}$ has at most $q$ nonzeros per column.

%put a weight matrix on the RHS. 
Take weighted combination of deep topics which are themselves combinations of basic topics.
\item
The number of free parameters is $dk_0+k_0q\ll dk$. 
\end{itemize}
%Ex. college: which college, then dept?
%Come up with examples where this is the right model.
In this model the names on the second level need to be the same.

Note the framework of topic modeling requires a ground truth. %hierarchical, what the goal is. 
%correlated topic model
%nested
%hierarchical.

%We assume there is a dominant, 2nd dominant,... $q$th dominant. We assume weight is falling off.

We test on PubMed, Wikipedia, ProductAds, with $k_0=2000$, $k=5K, 10K, 20K, 50K, 100K$. 

We do well on coherence (do words found cooccur? for each document take top 5 words, for each pair within here, see how many times they occur together), not so well on complexity.

%100k likelihood too large? 100-paper topics
%10 million.
% a lot of deep topics don't have rep docs?







